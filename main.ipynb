{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T13:06:41.232618700Z",
     "start_time": "2023-10-27T13:06:39.626246500Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# LIBRARIES IMPORTS #\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe\n",
    "\n",
    "The dataset provided is composed of a \"lighter_authors.json\" file of about 0.5 Gbs and a \"lighter_books.json\" file of about 15 Gbs. Considering that where will be a data-type conversion the dataset will become even bigger when loaded on pandas and they could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the result.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both this approach are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset lighter by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T13:07:00.338184800Z",
     "start_time": "2023-10-27T13:06:45.109131100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(\"datasets/lighter_authors.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:26:36.785448100Z",
     "start_time": "2023-10-19T14:26:36.722476100Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T11:19:42.295930600Z",
     "start_time": "2023-10-26T11:19:42.068842300Z"
    }
   },
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:26:46.912457700Z",
     "start_time": "2023-10-19T14:26:46.528663500Z"
    }
   },
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:07:17.320382400Z",
     "start_time": "2023-10-19T11:07:17.245410100Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\" )\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T13:07:06.963400500Z",
     "start_time": "2023-10-27T13:07:06.249511700Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] CHANGE DATA TYPES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T12:59:54.547590200Z",
     "start_time": "2023-10-23T12:59:53.518546400Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(\"datasets/lighter_books.json\", lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\" )\n",
    "print(\"The 'about' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns such as \"image_url\", \"isb\", \"isbn13\", \"asin\" [TODO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:00:05.461282300Z",
     "start_time": "2023-10-23T13:00:05.192095600Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:24:13.041680400Z",
     "start_time": "2023-10-23T13:12:38.419475900Z"
    }
   },
   "outputs": [],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "col_to_drop = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\"]\n",
    " \n",
    "for chunk in chunks:\n",
    "    chunk.drop(columns = col_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:08:47.740523800Z",
     "start_time": "2023-10-23T13:08:47.698186100Z"
    }
   },
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:09:15.518507400Z",
     "start_time": "2023-10-23T13:08:54.523566900Z"
    }
   },
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:09:23.955740200Z",
     "start_time": "2023-10-23T13:09:23.914078700Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ1] Exploratory Data Analysis (EDA)\n",
    "\n",
    "TODO\n",
    "\n",
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ2] Let’s finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T09:40:04.500894600Z",
     "start_time": "2023-10-22T09:40:04.490003600Z"
    }
   },
   "outputs": [],
   "source": [
    "for elem in books[\"num_pages\"]:\n",
    "    if type(elem) != type(100):\n",
    "        print(\"TROVATO\")\n",
    "        print(elem)\n",
    "        print(type(elem))\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#books[books[\"num_pages\"] > 250].shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "*Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender, and comment about anything eventually related to “structural bias.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 5.1.a:** Plot the top 10 most influential authors regarding their fan count and number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:24:32.295626600Z",
     "start_time": "2023-10-23T13:24:31.422950800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count the actual number of books written by every author using the length of the list of his book ids.\n",
    "authors[\"book_count\"] = authors[\"work_ids\"].apply(len)\n",
    "\n",
    "#10  most influential authors regarding their fan count\n",
    "x = authors.nlargest(10, \"fans_count\")\n",
    "\n",
    "#plot the data\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(x[\"name\"], x[\"fans_count\"], color = 'green', label = 'Fan Count')\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(x[\"name\"], x[\"book_count\"], color = 'blue', label = 'Book Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 5.1.b:** Who is the most influential author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:24:36.730184800Z",
     "start_time": "2023-10-23T13:24:36.689570Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Most influential author\n",
    "\n",
    "m_inf = authors.nlargest(1, \"fans_count\")\n",
    "print(\"The most influential author, having by far the largest fanbase is:\", m_inf[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 5.2:** Have they published any series of books? If any, extract the longest series name among these authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:38:28.642841100Z",
     "start_time": "2023-10-23T13:38:28.389578100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a new dataframe containing only the books written by the top 10 authors\n",
    "top_authors_books = books[books[\"author_id\"].isin(list(x[\"id\"]))]\n",
    "\n",
    "#Get the series authors by their \"series position\" to find who has written series, and use max position to determine their longest series' length.\n",
    "\n",
    "author_series_lengths = top_authors_books.groupby([\"author_id\", \"author_name\"])[\"series_position\"].max().reset_index()\n",
    "#return the highest series position, thus the longest series author and name\n",
    "author_series = top_authors_books.groupby([\"author_id\",\"author_name\", \"series_name\"])[\"series_position\"].max().reset_index()\n",
    "max_series = author_series[author_series[\"series_position\"] == author_series[\"series_position\"].max()]\n",
    "\n",
    "\n",
    "print(\"Authors of series are:\")\n",
    "print(author_series_lengths[\"author_name\"])\n",
    "\n",
    "print(\"\\nThe author of the longest series\")\n",
    "print(max_series[\"author_name\"])\n",
    "print(\"And the series is:\")\n",
    "print(max_series[\"series_name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 5,2** How many of these authors have been published in different formats? Provide a meaningful chart on the distribution of the formats and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:42:54.329961Z",
     "start_time": "2023-10-23T13:42:51.970459100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To find the authors published in different formats from the top 10\n",
    "\n",
    "authors_with_different_formats = top_authors_books[top_authors_books.duplicated(subset=[\"author_id\"], keep=False)]\n",
    "\n",
    "#to avoid name repetition\n",
    "unique_author_ids = authors_with_different_formats[\"author_id\"].unique()\n",
    "\n",
    "# Create a separate chart for each author with multiple formats\n",
    "for author_id in unique_author_ids:\n",
    "    author_data = authors_with_different_formats[authors_with_different_formats[\"author_id\"] == author_id]\n",
    "    author_name = author_data[\"author_name\"].values[0]\n",
    "    if not author_data[\"format\"].isnull:\n",
    "        format_distribution = author_data[\"format\"].value_counts()\n",
    "    \n",
    "    format_distribution.plot(kind='bar')\n",
    "    plt.title(f\"Formats Distribution for {author_name}\")\n",
    "    plt.xlabel(\"Format\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Comments on the graphs above:\n",
    "\n",
    "We can see that the books are mainly published in paperback and hardcover formats, whereas the \"newer\" digital formats are very few. This would be due to 3 main reasons: \n",
    "1- the age of most of the books written by these authors since audiobooks and ebook formats are relatively new.\n",
    "2- even with more widely available digital formats now, these are authors of novels and readers who usually enjoy leisurely reading prefer the physical format of paper, rather than reading through a screen.\n",
    "3- Once a book has been published in a digital format, there is no need to republish it using another editorial, since it is already on the internet, a more universal \"bookstore\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "**Request 5.3:** Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender, and comment about anything eventually related to “structural bias.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# [RQ6] For this question, consider the top 10 authors concerning the number of fans again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "    Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?\n",
    "    For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 6.1:** Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T13:07:23.045862300Z",
     "start_time": "2023-10-27T13:07:22.170775Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 1: transform the publication date entry to datetime format\n",
    "\n",
    "top_authors_books[\"original_publication_date\"] = pd.to_datetime(top_authors_books[\"original_publication_date\"], format = 'mixed')\n",
    "\n",
    "#calculate time gap between consecutive \"original\" publications and add it into a column for all books\n",
    "top_authors_books[\"time_gap\"] = top_authors_books.groupby([\"author_id\", \"series_id\"])[\"original_publication_date\"].diff()\n",
    "\n",
    "# Calculate the average time gap for books in a series and not in a series, according to the new column created\n",
    "average_time_gap_series = top_authors_books[top_authors_books[\"series_id\"].notna()][\"time_gap\"].mean()\n",
    "average_time_gap_non_series = top_authors_books[top_authors_books[\"series_id\"].isna()][\"time_gap\"].mean()\n",
    "\n",
    "#print(f\"Average Time Gap for Books in a Series: {average_time_gap_series}\")\n",
    "#print(f\"Average Time Gap for Books Not in a Series: {average_time_gap_non_series}\")\n",
    "print(top_authors_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What do we expect to see?\n",
    "\n",
    "As expected the books that belong to a series have a more periodic and regular output from the authors(on average), and this logically makes sense following the idea that the author is breaking down a full story into separate books.\n",
    "Books that do not belong to a series however do not have a well-defined average gap between them since they are independent works and do not follow any periodic output \"expectation\" from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Request 6.2:** For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T15:10:33.058243300Z",
     "start_time": "2023-10-22T15:10:32.802266900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a new column using the datetime format of the original publication date\n",
    "top_authors_books['year'] = top_authors_books[\"original_publication_date\"].dt.year\n",
    "writers_counts = top_authors_books.groupby(['author_name', 'year']).size().reset_index(name='Count')\n",
    "\n",
    "# Plot the publication history for each author\n",
    "writers = writers_counts['author_name'].unique()\n",
    "for writer in writers:\n",
    "    author_data = writers_counts[writers_counts['author_name'] == writer]\n",
    "    plt.plot(author_data['year'], author_data['Count'], label=writer)\n",
    "\n",
    "plt.title(\"Publication History of Authors\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Books Published\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
