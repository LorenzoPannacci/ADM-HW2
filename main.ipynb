{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install pandas tqdm numpy matplotlib pyspark dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import dask.dataframe as dd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# GLOBAL PARAMETERS #\n",
    "#####################\n",
    "\n",
    "chunk_size = 100000 # 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe\n",
    "\n",
    "The dataset provided is composed of a \"lighter_authors.json\" file of about 0.5 Gbs and a \"lighter_books.json\" file of about 15 Gbs. Considering that where will be a data-type conversion the dataset will become even bigger when loaded on pandas and they could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the result.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both this approach are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset lighter by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(\"datasets/lighter_authors.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] CHANGE DATA TYPES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(\"datasets/lighter_books.json\", lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns such as \"image_url\", \"isb\", \"isbn13\", \"asin\" [TODO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunks_number = np.ceil(7027431 / chunk_size)\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "columns_to_drop = [\"author_name\", \"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\"]\n",
    "\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions [RQs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ1] Exploratory Data Analysis (EDA)\n",
    "\n",
    "TODO\n",
    "\n",
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column.\n",
    "\n",
    "Negative average ratings and ratings count and fans count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ2] Let’s finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?\n",
    "\n",
    "Notice that there are some entries that have a void string instead of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view that excludes the entries with void string\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "# execute query\n",
    "result = df[df[\"num_pages\"].astype(int) > 250].shape[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.1:** Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "* The number of books published that year.\n",
    "* The total number of pages written that year.\n",
    "* The most prolific month of that year.\n",
    "* The longest book written that year.\n",
    "\n",
    "We have to cope on the number of pages, how do we work in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "def look_by_year(books, year):\n",
    "    books_year = books[\"original_publication_date\" == year]\n",
    "    n_books = books_year.shape[0]\n",
    "\n",
    "    tot_pages = sum(books_year[\"num_pages\"]) #todo\n",
    "    prolific_month = \"\" # todo\n",
    "    longest_book = \"\" #todo\n",
    "\n",
    "    return n_books, tot_pages, prolific_month, longest_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.1:** You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[(authors[\"name\"] == authors[\"name\"]) & (authors[\"id\"] != authors[\"id\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.1:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload authors.json dataset, and filter authors with at least 100 reviews. Do the same using Pandas and compare performance in terms of milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_path = r\"D:\\Data Science\\ADM\\Datasets\\Books metadata dataset\\archive\\authors.json\\authors.json\"\n",
    "\n",
    "chunks_number = np.ceil(7374310 / chunk_size)\n",
    "columns_to_drop = [\"about\", \"image_url\"]\n",
    "\n",
    "# with reviews is it meant text reviews or ratings?\n",
    "\n",
    "authors = pd.DataFrame()\n",
    "chunks = pd.read_json(authors_path, lines = True, chunksize = chunk_size)\n",
    "\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    authors = pd.concat([authors, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PANDAS\n",
    "\n",
    "print(\"Measuring Pandas performance...\")\n",
    "%timeit authors[authors[\"text_reviews_count\"] >= 100]\n",
    "\n",
    "# IN DASK\n",
    "\n",
    "authors_dask = dd.from_pandas(authors, npartitions = 1)\n",
    "\n",
    "print(\"Measuring Dask performance...\")\n",
    "%timeit authors_dask[authors_dask[\"text_reviews_count\"] >= 100].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.2:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload books.json, and join them with authors.json based on author_id. How many books don’t have a match for the author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_path = r\"D:\\Data Science\\ADM\\Datasets\\Books metadata dataset\\archive\\books.json\\books.json\"\n",
    "\n",
    "chunks_number = np.ceil(1000000 / chunk_size)\n",
    "columns_to_keep = [\"id\", \"author_id\"]\n",
    "\n",
    "books = pd.DataFrame()\n",
    "chunks = pd.read_json(books_path, lines = True, chunksize = chunk_size)\n",
    "\n",
    "total_istances = 0\n",
    "authorless_istances = 0\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk = chunk[columns_to_keep]\n",
    "    total_istances += chunk.shape[0]\n",
    "\n",
    "    join = pd.merge(chunk, authors, left_on = \"author_id\", right_on = \"id\", how = \"left\")\n",
    "    authorless_istances += sum(join[\"id_x\"].isna())\n",
    "    print(authorless_istances)\n",
    "\n",
    "print(\"Total instances: \", total_istances)\n",
    "print(\"Authorless istances: \", authorless_istances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = pd.merge(chunk, authors, left_on = \"author_id\", right_on = \"id\", how = \"left\")\n",
    "sum(join[\"id_x\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Every book should have a field named description, and any author should have a field named description. Choose one of the two and perform a text-mining analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** If you choose to text-mine books.json ’ descriptions, try to find a way to group books in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:** If you choose to text-mine authors.json’ about-field, try to find a way to group authors in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** If you feel comfortable and did both tasks, analyze the matching of the two procedures. You grouped books and authors in genres. Do these two procedures show correspondence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Question (CLQ)\n",
    "\n",
    "Using the command line is a feature that Data Scientists must master. It is relevant since the operations there require less memory to use in comparison to other interfaces. It also uses less CPU processing time than other interfaces. In addition, it can be faster and more efficient and handle repetitive tasks quickly.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response.\n",
    "\n",
    "Looking through the files, you can find series.json, which contains a list of book series. In each series's 'works' field, you'll find a list of books that are part of that series. Report the title of the top 5 series with the highest total 'books_count' among all of their associated books using command line tools.\n",
    "\n",
    "1. Write a script to provide this report. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_original.[put_the_proper extension]\n",
    "\n",
    "2. Try interacting with ChatGPT or any other LLM chatbot tool to implement a more robust script implementation. Your final script should be at most three lines. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_LLM.[put_the_proper_ extension]. Add in your homework how you employed the LLM chatbot tools, validate if it is correct, and explain how you check its correctness.\n",
    "\n",
    "The expected result is as follows:\n",
    "\n",
    "| id    | title                                | total_books_count |\n",
    "|-------|--------------------------------------|-------------------|\n",
    "| 302380| Extraordinary Voyages                | 20138             |\n",
    "| 94209 | Alice's Adventures in Wonderland     | 14280             |\n",
    "| 311348| Kolekcja Arcydzieł Literatury Światowe| 13774             |\n",
    "| 41459 | Oz                                   | 11519             |\n",
    "| 51138 | Hercule Poirot                       | 11305             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1:**\n",
    "\n",
    "The shell script file *commandline_original.sh* is implemented as requested and the output correspond to what it's expected, the only difference we find it's the transformation of special characters like 'Ś' in 'Światowe' to their unicode counterpart, as '\\u015'. Comment on the code is inside the file itself but we briefly describe the code here: for each line of the json file we extract 'id' and 'title' of the series and than the whole \"works\" subsection using Regexs. From the \"works\" subsection we search every field 'work_counts', sum the value, store it in an accumulator and append everything in a list. After having gone through every line of the json file we sort the list using only the total work counts field and using a for cycle print the first five results following the format of the table shown.\n",
    "\n",
    "**Request 2:**\n",
    "\n",
    "We tried to use ChatGPT 3.5 to implement a more robust script implementation that is also at most three lines. This is the prompt sent:\n",
    "\n",
    "```\n",
    "You have to write a more robust implementation of the given bash code. Your implementation must be at most three lines long. You can't use jq.\n",
    "[original sh code here]\n",
    "```\n",
    "\n",
    "We explicitely said not to use jq to avoid using external libraries. The result is stored in *commandline_LLM.sh* and it doesn't work. We can immediately see it from the different output and the fact that in the source code it's never mentioned the field \"books_count\". We also tried to use alternative LLMs chatbots like Bing with similar results. Removing the costraint of using only three lines the results become correct but also extremely similar to the original code. Meanwhile giving only the text of the question does not bring positive results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Question (AWSQ)\n",
    "\n",
    "AWS offers access to many cloud-based tools and services that simplify data processing, storage, and analysis. Thanks to AWS's scalable and affordable solutions, data scientists can work effectively with large datasets and carry out advanced analytics. A data scientist must, therefore, perform the essential task of learning how to use AWS. To complete a straightforward data analysis task in this question, you must set up an environment on Amazon Web Services.\n",
    "\n",
    "In this question, you are asked to provide the most commonly used tags for book lists. Going through the list.json file, you'll notice that each list has a list of tags attached, and we want to see what are the most popular tags across all of the lists. Please report the top 5 most frequently used tags and the number of times they appear in the lists.\n",
    "\n",
    "You have to follow the following (recommended) steps:\n",
    "\n",
    "* Download the list.json file to your local system.\n",
    "* Write a Python script that generates the report and the system's time to generate it.\n",
    "* Set up an EC2 instance on your AWS account and upload the list.json file together with your script to the instance\n",
    "* Compare the running times of your script on your local system and the EC2 instances.\n",
    "\n",
    "**Important note:** Please run the same script on both your local system and your EC2 instance to compare the results. e.g., * keep the parameters the same if you are processing the data by loading it partially and aggregating the results. Comment about the differences you find.\n",
    "\n",
    "Please provide a report as follows:\n",
    "\n",
    "* The information about the config of the EC2 instance\n",
    "* The command used to connect to the EC2\n",
    "* The commands used to upload the files and run the script on the EC2 instance through your local system\n",
    "* A table containing the most popular tags and their number of usage\n",
    "* A table containing the running time of the script on your local system and EC2 instance\n",
    "\n",
    "The following is the expected outcome for the most popular tags:\n",
    "\n",
    "|     Tag            |   # Usage   |\n",
    "|--------------------|------------|\n",
    "| romance            |     6001    |\n",
    "| fiction            |     5291    |\n",
    "| young-adult        |     5016    |\n",
    "| fantasy            |     3666    |\n",
    "| science-fiction    |     2779    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Question (AQ)\n",
    "\n",
    "Assume you are working as a librarian at a public library in Rome. Some new books have arrived, and you are in charge of putting them on a shelf. Your supervisor will give you some instructions, and you will follow his. Each book has a unique ID, and your boss's instructions can be of the following types:\n",
    "\n",
    "* L N - place the book with ID = N on the shelf to the left of the leftmost existing book\n",
    "* R N - place the book with ID = N on the shelf to the right of the rightmost existing book\n",
    "* ? N - Calculate the minimum number of books you must pop from the left or right to have the book with ID = N as the leftmost or rightmost book on the shelf.\n",
    "\n",
    "You must follow your boss's instructions and report the answers to type 3 instructions to him. He guarantees that if he has a type 3 instruction for a book with a specific ID, the book has already been placed on the shelf.\n",
    "\n",
    "Remember that once you've answered a type 3 instruction, the order of the books does not change.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "The first line contains a single number, n, representing the number of your boss's instructions. The ith instruction the boss gives can be found at each of the following n lines.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Print your boss's type 3 instructions in the order they appear in the input.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Input 1\n",
    "```\n",
    "L 75\n",
    "R 20\n",
    "R 30\n",
    "L 11\n",
    "? 75\n",
    "L 12\n",
    "L 15\n",
    "? 20\n",
    "```\n",
    "\n",
    "Output 1\n",
    "```\n",
    "1\n",
    "1\n",
    "```\n",
    "\n",
    "Input 2\n",
    "```\n",
    "R 1\n",
    "L 2\n",
    "L 3\n",
    "L 4\n",
    "? 3\n",
    "R 5\n",
    "R 6\n",
    "L 7\n",
    "L 8\n",
    "? 4\n",
    "L 9\n",
    "R 10\n",
    "R 11\n",
    "L 12\n",
    "L 13\n",
    "? 11\n",
    "? 3\n",
    "```\n",
    "\n",
    "Output 2:\n",
    "```\n",
    "1\n",
    "2\n",
    "0\n",
    "6\n",
    "```\n",
    "\n",
    "1. Implement a code to answer the problem above.\n",
    "\n",
    "2. Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Do you believe this is correct? How can you double-check it? Elaborate about your answer.\n",
    "\n",
    "3. Is the algorithm proposed in (1.) the optimal one to produce the required output? If not, can you suggest a better algorithm to perform the same task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithmic_question(input_string):\n",
    "    input_list = input_string.split(\"\\n\")\n",
    "    n_commands = int(input_list.pop(0))\n",
    "\n",
    "    shelf = []\n",
    "    output = []\n",
    "    for i in range(n_commands):\n",
    "        command = input_list[i].split(\" \")\n",
    "        if command[0] == \"L\":\n",
    "            shelf = [command[1]] + shelf\n",
    "        elif command[0] == \"R\":\n",
    "            shelf = shelf + [command[1]]\n",
    "        elif command[0] == \"?\":\n",
    "            index = shelf.index(command[1])\n",
    "            result = min(index, (len(shelf) - 1) - index)\n",
    "            output.append(result)\n",
    "\n",
    "    print(*output, sep = \"\\n\")\n",
    "    return\n",
    " \n",
    "input1 = \"8\\nL 75\\nR 20\\nR 30\\nL 11\\n? 75\\nL 12\\nL 15\\n? 20\\n\"\n",
    "print(\"Test1:\")\n",
    "algorithmic_question(input1)\n",
    "\n",
    "\n",
    "input2 = \"17\\nR 1\\nL 2\\nL 3\\nL 4\\n? 3\\nR 5\\nR 6\\nL 7\\nL 8\\n? 4\\nL 9\\nR 10\\nR 11\\nL 12\\nL 13\\n? 11\\n? 3\\n\"\n",
    "print(\"\\nTest2:\")\n",
    "algorithmic_question(input2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
