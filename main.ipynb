{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# LIBRARIES IMPORTS #\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe\n",
    "\n",
    "The dataset provided is composed of a \"lighter_authors.json\" file of about 0.5 Gbs and a \"lighter_books.json\" file of about 15 Gbs. Considering that where will be a data-type conversion the dataset will become even bigger when loaded on pandas and they could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the result.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both this approach are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset lighter by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(\"datasets/lighter_authors.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] CHANGE DATA TYPES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(\"datasets/lighter_books.json\", lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns such as \"image_url\", \"isb\", \"isbn13\", \"asin\" [TODO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunk_size = 100000 # 10000\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "columns_to_drop = [\"author_name\", \"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\"]\n",
    "\n",
    "for chunk in tqdm(chunks, total = 71): # 703\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ1] Exploratory Data Analysis (EDA)\n",
    "\n",
    "TODO\n",
    "\n",
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column.\n",
    "\n",
    "Negative average ratings and ratings count and fans count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ2] Let’s finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?\n",
    "\n",
    "Notice that there are some entries that have a void string instead of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view that excludes the entries with void string\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "# execute query\n",
    "result = df[df[\"num_pages\"].astype(int) > 250].shape[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.1:** Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "* The number of books published that year.\n",
    "* The total number of pages written that year.\n",
    "* The most prolific month of that year.\n",
    "* The longest book written that year.\n",
    "\n",
    "We have to cope on the number of pages, how do we work in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_by_year(books, year):\n",
    "    books_year = books[\"original_publication_date\" == year]\n",
    "    n_books = books_year.shape[0]\n",
    "\n",
    "    tot_pages = sum(books_year[\"num_pages\"]) #todo\n",
    "    prolific_month = # todo\n",
    "    longest_book = #todo\n",
    "\n",
    "    return n_books, tot_pages, prolific_month, longest_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
