{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# LIBRARIES IMPORTS #\n",
    "#####################\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe\n",
    "\n",
    "The dataset provided is composed of a \"lighter_authors.json\" file of about 0.5 Gbs and a \"lighter_books.json\" file of about 15 Gbs. Considering that where will be a data-type conversion the dataset will become even bigger when loaded on pandas and they could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the result.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both this approach are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset lighter by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(\"datasets/lighter_authors.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] CHANGE DATA TYPES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(\"datasets/lighter_books.json\", lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns such as \"image_url\", \"isb\", \"isbn13\", \"asin\" [TODO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunk_size = 100000 # 10000\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "columns_to_drop = [\"author_name\", \"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\"]\n",
    "\n",
    "for chunk in tqdm(chunks, total = 71): # 703\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions [RQs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ1] Exploratory Data Analysis (EDA)\n",
    "\n",
    "TODO\n",
    "\n",
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column.\n",
    "\n",
    "Negative average ratings and ratings count and fans count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ2] Let’s finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?\n",
    "\n",
    "Notice that there are some entries that have a void string instead of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view that excludes the entries with void string\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "# execute query\n",
    "result = df[df[\"num_pages\"].astype(int) > 250].shape[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.1:** Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "* The number of books published that year.\n",
    "* The total number of pages written that year.\n",
    "* The most prolific month of that year.\n",
    "* The longest book written that year.\n",
    "\n",
    "We have to cope on the number of pages, how do we work in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "def look_by_year(books, year):\n",
    "    books_year = books[\"original_publication_date\" == year]\n",
    "    n_books = books_year.shape[0]\n",
    "\n",
    "    tot_pages = sum(books_year[\"num_pages\"]) #todo\n",
    "    prolific_month = \"\" # todo\n",
    "    longest_book = \"\" #todo\n",
    "\n",
    "    return n_books, tot_pages, prolific_month, longest_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.1:** You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>fans_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ratings_count, average_rating, text_reviews_count, work_ids, book_ids, works_count, id, name, gender, fans_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[(authors[\"name\"] == authors[\"name\"]) & (authors[\"id\"] != authors[\"id\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Question (CLQ)\n",
    "\n",
    "Using the command line is a feature that Data Scientists must master. It is relevant since the operations there require less memory to use in comparison to other interfaces. It also uses less CPU processing time than other interfaces. In addition, it can be faster and more efficient and handle repetitive tasks quickly.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response.\n",
    "\n",
    "Looking through the files, you can find series.json, which contains a list of book series. In each series's 'works' field, you'll find a list of books that are part of that series. Report the title of the top 5 series with the highest total 'books_count' among all of their associated books using command line tools.\n",
    "\n",
    "1. Write a script to provide this report. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_original.[put_the_proper extension]\n",
    "\n",
    "2. Try interacting with ChatGPT or any other LLM chatbot tool to implement a more robust script implementation. Your final script should be at most three lines. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_LLM.[put_the_proper_ extension]. Add in your homework how you employed the LLM chatbot tools, validate if it is correct, and explain how you check its correctness.\n",
    "\n",
    "The expected result is as follows:\n",
    "\n",
    "| id    | title                                | total_books_count |\n",
    "|-------|--------------------------------------|-------------------|\n",
    "| 302380| Extraordinary Voyages                | 20138             |\n",
    "| 94209 | Alice's Adventures in Wonderland     | 14280             |\n",
    "| 311348| Kolekcja Arcydzieł Literatury Światowe| 13774             |\n",
    "| 41459 | Oz                                   | 11519             |\n",
    "| 51138 | Hercule Poirot                       | 11305             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
