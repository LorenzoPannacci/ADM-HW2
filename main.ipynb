{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download needed libraries if not already installed, then we import them and set some useful global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:34:13.023173400Z",
     "start_time": "2023-10-31T15:33:10.771357300Z"
    }
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install pandas tqdm numpy matplotlib pyspark dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:36:39.700441800Z",
     "start_time": "2023-11-02T13:36:38.086227700Z"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import re\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:36:40.938212800Z",
     "start_time": "2023-11-02T13:36:40.912587300Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# GLOBAL PARAMETERS #\n",
    "#####################\n",
    "\n",
    "chunk_size = 100000 # 10000\n",
    "\n",
    "authors_path = r\"datasets/lighter_authors.json\"\n",
    "books_path = r\"datasets/lighter_books.json\"\n",
    "list_path = r\"datasets/list.json\"\n",
    "series_path = r\"datasets/series.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided is composed of 4 .json files:\n",
    "* authors.json\n",
    "* books.json\n",
    "* list.json\n",
    "* series.json\n",
    "\n",
    "While the two latter are manageable the first two are very big and difficult to use. Instead of the whole files, for those two are provided lighter versions: \"lighter_authors.json\" is about 0.5 Gbs and \"lighter_books.json\" is 15 Gbs. Considering that where will be a data-type conversions the dataset will become even bigger when loaded on pandas and could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the results.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both those approaches are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset as light as possible by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:37:06.272364500Z",
     "start_time": "2023-11-02T13:36:55.844860600Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(authors_path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:01.466520900Z",
     "start_time": "2023-10-31T15:35:01.434489800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>work_ids</th>\n",
       "      <th>book_ids</th>\n",
       "      <th>works_count</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>image_url</th>\n",
       "      <th>about</th>\n",
       "      <th>fans_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2862064</td>\n",
       "      <td>4.19</td>\n",
       "      <td>62681</td>\n",
       "      <td>[3078186, 135328, 1877624, 74123, 3078120, 104...</td>\n",
       "      <td>[386162, 13, 8695, 8694, 6091075, 365, 569429,...</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/159137433...</td>\n",
       "      <td>Douglas NoÃ«l Adams was an English author, comi...</td>\n",
       "      <td>19826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1417316</td>\n",
       "      <td>4.02</td>\n",
       "      <td>84176</td>\n",
       "      <td>[613469, 2305997, 940892, 2611786, 7800569, 31...</td>\n",
       "      <td>[9791, 21, 28, 24, 7507825, 27, 10538, 25, 26,...</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>Bill Bryson</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/157859752...</td>\n",
       "      <td>William McGuire \"Bill\" Bryson, OBE, FRS was bo...</td>\n",
       "      <td>16144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56159</td>\n",
       "      <td>4.53</td>\n",
       "      <td>352</td>\n",
       "      <td>[17150, 808427, 20487307, 90550, 25460625, 171...</td>\n",
       "      <td>[349254, 15222, 14833682, 15221, 18126815, 152...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>Jude Fisher</td>\n",
       "      <td>female</td>\n",
       "      <td>https://images.gr-assets.com/authors/141145711...</td>\n",
       "      <td>Jude Fisher is the pseudonym for &lt;a href=\"http...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3302</td>\n",
       "      <td>3.79</td>\n",
       "      <td>480</td>\n",
       "      <td>[4417, 14300808, 14780, 3796968, 44703121, 103...</td>\n",
       "      <td>[40, 9416484, 12482, 3753106, 26889789, 104764...</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>James Hamilton-Paterson</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/127051738...</td>\n",
       "      <td>James Hamilton-Paterson's work has been transl...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7979</td>\n",
       "      <td>3.60</td>\n",
       "      <td>772</td>\n",
       "      <td>[13330815, 19109351, 42306244, 72694240, 26291...</td>\n",
       "      <td>[8466327, 15739968, 22756778, 51026133, 260451...</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "      <td>Mark Watson</td>\n",
       "      <td>male</td>\n",
       "      <td>https://images.gr-assets.com/authors/133175379...</td>\n",
       "      <td>Mark Andrew Watson (born 13 February 1980) is ...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings_count  average_rating  text_reviews_count  \\\n",
       "0        2862064            4.19               62681   \n",
       "1        1417316            4.02               84176   \n",
       "2          56159            4.53                 352   \n",
       "3           3302            3.79                 480   \n",
       "4           7979            3.60                 772   \n",
       "\n",
       "                                            work_ids  \\\n",
       "0  [3078186, 135328, 1877624, 74123, 3078120, 104...   \n",
       "1  [613469, 2305997, 940892, 2611786, 7800569, 31...   \n",
       "2  [17150, 808427, 20487307, 90550, 25460625, 171...   \n",
       "3  [4417, 14300808, 14780, 3796968, 44703121, 103...   \n",
       "4  [13330815, 19109351, 42306244, 72694240, 26291...   \n",
       "\n",
       "                                            book_ids  works_count  id  \\\n",
       "0  [386162, 13, 8695, 8694, 6091075, 365, 569429,...          106   4   \n",
       "1  [9791, 21, 28, 24, 7507825, 27, 10538, 25, 26,...           75   7   \n",
       "2  [349254, 15222, 14833682, 15221, 18126815, 152...           14  10   \n",
       "3  [40, 9416484, 12482, 3753106, 26889789, 104764...           45  12   \n",
       "4  [8466327, 15739968, 22756778, 51026133, 260451...           61  14   \n",
       "\n",
       "                      name  gender  \\\n",
       "0            Douglas Adams    male   \n",
       "1              Bill Bryson    male   \n",
       "2              Jude Fisher  female   \n",
       "3  James Hamilton-Paterson    male   \n",
       "4              Mark Watson    male   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://images.gr-assets.com/authors/159137433...   \n",
       "1  https://images.gr-assets.com/authors/157859752...   \n",
       "2  https://images.gr-assets.com/authors/141145711...   \n",
       "3  https://images.gr-assets.com/authors/127051738...   \n",
       "4  https://images.gr-assets.com/authors/133175379...   \n",
       "\n",
       "                                               about  fans_count  \n",
       "0  Douglas NoÃ«l Adams was an English author, comi...       19826  \n",
       "1  William McGuire \"Bill\" Bryson, OBE, FRS was bo...       16144  \n",
       "2  Jude Fisher is the pseudonym for <a href=\"http...          60  \n",
       "3  James Hamilton-Paterson's work has been transl...          72  \n",
       "4  Mark Andrew Watson (born 13 February 1980) is ...         179  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:01.922228400Z",
     "start_time": "2023-10-31T15:35:01.455012400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351767 entries, 0 to 351766\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ratings_count       351767 non-null  int64  \n",
      " 1   average_rating      351767 non-null  float64\n",
      " 2   text_reviews_count  351767 non-null  int64  \n",
      " 3   work_ids            351767 non-null  object \n",
      " 4   book_ids            351767 non-null  object \n",
      " 5   works_count         351767 non-null  int64  \n",
      " 6   id                  351767 non-null  int64  \n",
      " 7   name                351767 non-null  object \n",
      " 8   gender              351767 non-null  object \n",
      " 9   image_url           351767 non-null  object \n",
      " 10  about               351767 non-null  object \n",
      " 11  fans_count          351767 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 32.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# check infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:02.095198100Z",
     "start_time": "2023-10-31T15:35:01.710820900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                       132\n",
       "ratings_count           2814136\n",
       "average_rating          2814136\n",
       "text_reviews_count      2814136\n",
       "work_ids               80361120\n",
       "book_ids               80361120\n",
       "works_count             2814136\n",
       "id                      2814136\n",
       "name                   26225127\n",
       "gender                 21473021\n",
       "image_url              44826054\n",
       "about                 410897820\n",
       "fans_count              2814136\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:02.110826500Z",
     "start_time": "2023-10-31T15:35:02.085585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset just as imported uses 0.63 GBs of RAM!\n",
      "The 'about' column covers 60.0 % of the total RAM usage alone!\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:37:14.180116300Z",
     "start_time": "2023-11-02T13:37:13.747013800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset now uses 0.13 GBs of RAM!\n"
     ]
    }
   ],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\", \"work_ids\", \"works_count\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:14.777559400Z",
     "start_time": "2023-10-31T15:35:14.207203600Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(books_path, lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:14.933069900Z",
     "start_time": "2023-10-31T15:35:14.869509600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>asin</th>\n",
       "      <th>language</th>\n",
       "      <th>...</th>\n",
       "      <th>format</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>image_url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_name</th>\n",
       "      <th>series_position</th>\n",
       "      <th>shelves</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>[{'id': '1077326', 'name': 'J.K. Rowling', 'ro...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1077326</td>\n",
       "      <td>2809203</td>\n",
       "      <td>0439358078</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td></td>\n",
       "      <td>eng</td>\n",
       "      <td>...</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>US Edition</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>870</td>\n",
       "      <td>45175</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'name': 'to-read', 'count': 324191}, {'name'...</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>[{'id': '1077326', 'name': 'J.K. Rowling', 'ro...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1077326</td>\n",
       "      <td>4640799</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eng</td>\n",
       "      <td>...</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>Library Edition</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Scholastic Inc</td>\n",
       "      <td>309</td>\n",
       "      <td>45175</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'name': 'fantasy', 'count': 63540}, {'name':...</td>\n",
       "      <td>Harry Potter's life is miserable. His parents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>None</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1077326</td>\n",
       "      <td>6231171</td>\n",
       "      <td>0439554896</td>\n",
       "      <td>9780439554893</td>\n",
       "      <td></td>\n",
       "      <td>eng</td>\n",
       "      <td>...</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td></td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>352</td>\n",
       "      <td>45175</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'name': 'to-read', 'count': 282341}, {'name'...</td>\n",
       "      <td>The Dursleys were so mean and hideous that sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>[{'id': '1077326', 'name': 'J.K. Rowling', 'ro...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1077326</td>\n",
       "      <td>2402163</td>\n",
       "      <td>043965548X</td>\n",
       "      <td>9780439655484</td>\n",
       "      <td></td>\n",
       "      <td>eng</td>\n",
       "      <td>...</td>\n",
       "      <td>Mass Market Paperback</td>\n",
       "      <td></td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>435</td>\n",
       "      <td>45175</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'to-read', 'count': 292815}, {'name'...</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>[{'id': '1077326', 'name': 'J.K. Rowling', 'ro...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1077326</td>\n",
       "      <td>3046572</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eng</td>\n",
       "      <td>...</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>First Scholastic Trade Paperback Edition</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>734</td>\n",
       "      <td>45175</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'name': 'to-read', 'count': 287086}, {'name'...</td>\n",
       "      <td>Harry Potter is midway through his training as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   2  Harry Potter and the Order of the Phoenix (Har...   \n",
       "1   3  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "2   4  Harry Potter and the Chamber of Secrets (Harry...   \n",
       "3   5  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "4   6  Harry Potter and the Goblet of Fire (Harry Pot...   \n",
       "\n",
       "                                             authors   author_name  author_id  \\\n",
       "0  [{'id': '1077326', 'name': 'J.K. Rowling', 'ro...  J.K. Rowling    1077326   \n",
       "1  [{'id': '1077326', 'name': 'J.K. Rowling', 'ro...  J.K. Rowling    1077326   \n",
       "2                                               None  J.K. Rowling    1077326   \n",
       "3  [{'id': '1077326', 'name': 'J.K. Rowling', 'ro...  J.K. Rowling    1077326   \n",
       "4  [{'id': '1077326', 'name': 'J.K. Rowling', 'ro...  J.K. Rowling    1077326   \n",
       "\n",
       "   work_id        isbn         isbn13 asin language  ...  \\\n",
       "0  2809203  0439358078  9780439358071           eng  ...   \n",
       "1  4640799                                      eng  ...   \n",
       "2  6231171  0439554896  9780439554893           eng  ...   \n",
       "3  2402163  043965548X  9780439655484           eng  ...   \n",
       "4  3046572                                      eng  ...   \n",
       "\n",
       "                  format                       edition_information  \\\n",
       "0              Paperback                                US Edition   \n",
       "1              Hardcover                           Library Edition   \n",
       "2              Hardcover                                             \n",
       "3  Mass Market Paperback                                             \n",
       "4              Paperback  First Scholastic Trade Paperback Edition   \n",
       "\n",
       "                                           image_url        publisher  \\\n",
       "0  https://i.gr-assets.com/images/S/compressed.ph...  Scholastic Inc.   \n",
       "1  https://i.gr-assets.com/images/S/compressed.ph...   Scholastic Inc   \n",
       "2  https://i.gr-assets.com/images/S/compressed.ph...       Scholastic   \n",
       "3  https://i.gr-assets.com/images/S/compressed.ph...  Scholastic Inc.   \n",
       "4  https://i.gr-assets.com/images/S/compressed.ph...       Scholastic   \n",
       "\n",
       "  num_pages series_id   series_name series_position  \\\n",
       "0       870     45175  Harry Potter               5   \n",
       "1       309     45175  Harry Potter               1   \n",
       "2       352     45175  Harry Potter               2   \n",
       "3       435     45175  Harry Potter               3   \n",
       "4       734     45175  Harry Potter               4   \n",
       "\n",
       "                                             shelves  \\\n",
       "0  [{'name': 'to-read', 'count': 324191}, {'name'...   \n",
       "1  [{'name': 'fantasy', 'count': 63540}, {'name':...   \n",
       "2  [{'name': 'to-read', 'count': 282341}, {'name'...   \n",
       "3  [{'name': 'to-read', 'count': 292815}, {'name'...   \n",
       "4  [{'name': 'to-read', 'count': 287086}, {'name'...   \n",
       "\n",
       "                                         description  \n",
       "0  There is a door at the end of a silent corrido...  \n",
       "1  Harry Potter's life is miserable. His parents ...  \n",
       "2  The Dursleys were so mean and hideous that sum...  \n",
       "3  For twelve long years, the dread fortress of A...  \n",
       "4  Harry Potter is midway through his training as...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:15.752574700Z",
     "start_time": "2023-10-31T15:35:15.679267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 26 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         9999 non-null   int64  \n",
      " 1   title                      9999 non-null   object \n",
      " 2   authors                    3742 non-null   object \n",
      " 3   author_name                9999 non-null   object \n",
      " 4   author_id                  9999 non-null   int64  \n",
      " 5   work_id                    9999 non-null   int64  \n",
      " 6   isbn                       9999 non-null   object \n",
      " 7   isbn13                     9999 non-null   object \n",
      " 8   asin                       9999 non-null   object \n",
      " 9   language                   9999 non-null   object \n",
      " 10  average_rating             9999 non-null   float64\n",
      " 11  rating_dist                9999 non-null   object \n",
      " 12  ratings_count              9999 non-null   int64  \n",
      " 13  text_reviews_count         9999 non-null   int64  \n",
      " 14  publication_date           9999 non-null   object \n",
      " 15  original_publication_date  9999 non-null   object \n",
      " 16  format                     9999 non-null   object \n",
      " 17  edition_information        9999 non-null   object \n",
      " 18  image_url                  9999 non-null   object \n",
      " 19  publisher                  9999 non-null   object \n",
      " 20  num_pages                  9999 non-null   object \n",
      " 21  series_id                  9999 non-null   object \n",
      " 22  series_name                9999 non-null   object \n",
      " 23  series_position            9999 non-null   object \n",
      " 24  shelves                    9999 non-null   object \n",
      " 25  description                9999 non-null   object \n",
      "dtypes: float64(1), int64(5), object(20)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:16.632184800Z",
     "start_time": "2023-10-31T15:35:16.554758600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                             132\n",
       "id                              79992\n",
       "title                          929445\n",
       "authors                        515672\n",
       "author_name                    713677\n",
       "author_id                       79992\n",
       "work_id                         79992\n",
       "isbn                           667641\n",
       "isbn13                         697223\n",
       "asin                           589989\n",
       "language                       596758\n",
       "average_rating                  79992\n",
       "rating_dist                    972520\n",
       "ratings_count                   79992\n",
       "text_reviews_count              79992\n",
       "publication_date               654329\n",
       "original_publication_date      638162\n",
       "format                         655109\n",
       "edition_information            610372\n",
       "image_url                     1484359\n",
       "publisher                      710464\n",
       "num_pages                      401847\n",
       "series_id                      598856\n",
       "series_name                    634022\n",
       "series_position                588928\n",
       "shelves                       6070816\n",
       "description                  10262905\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:17.554331500Z",
     "start_time": "2023-10-31T15:35:17.545014700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset just as imported uses 0.03 GBs of RAM!\n",
      "The 'description' column covers 35.0 % of the total RAM usage alone!\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'description' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns, where we consider a column useless when no exercise require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:35:22.005109100Z",
     "start_time": "2023-10-31T15:35:21.910651400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset now uses 0.01 GBs of RAM!\n"
     ]
    }
   ],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\", \"publication_date\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:45:13.709696600Z",
     "start_time": "2023-11-02T13:37:28.941241400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7f6e4a56784685897df411b01fe9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunks_number = np.ceil(7027431 / chunk_size)\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "columns_to_drop = [\"work_id\", \"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\", \"publication_date\"]\n",
    "\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:24:26.371109100Z",
     "start_time": "2023-10-31T16:24:26.332168500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7027431 entries, 0 to 7027430\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   id                         int64  \n",
      " 1   title                      object \n",
      " 2   authors                    object \n",
      " 3   author_name                object \n",
      " 4   author_id                  int64  \n",
      " 5   language                   object \n",
      " 6   average_rating             float64\n",
      " 7   rating_dist                object \n",
      " 8   ratings_count              int64  \n",
      " 9   text_reviews_count         int64  \n",
      " 10  original_publication_date  object \n",
      " 11  format                     object \n",
      " 12  num_pages                  object \n",
      " 13  series_id                  object \n",
      " 14  series_name                object \n",
      " 15  series_position            object \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 857.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:24:46.210615400Z",
     "start_time": "2023-10-31T16:24:27.871018200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                              132\n",
       "id                            56219448\n",
       "title                        684810705\n",
       "authors                      293131784\n",
       "author_name                  510420434\n",
       "author_id                     56219448\n",
       "language                     417560422\n",
       "average_rating                56219448\n",
       "rating_dist                  630673989\n",
       "ratings_count                 56219448\n",
       "text_reviews_count            56219448\n",
       "original_publication_date    454730116\n",
       "format                       447158221\n",
       "num_pages                    323639741\n",
       "series_id                    420869401\n",
       "series_name                  445383460\n",
       "series_position              413558394\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset uses 4.96 GBs of RAM!\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions [RQs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ1] Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column.\n",
    "\n",
    "Negative average ratings and ratings count and fans count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>id</th>\n",
       "      <th>fans_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.517670e+05</td>\n",
       "      <td>351767.000000</td>\n",
       "      <td>351767.000000</td>\n",
       "      <td>3.517670e+05</td>\n",
       "      <td>351767.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.770586e+03</td>\n",
       "      <td>3.651194</td>\n",
       "      <td>330.993243</td>\n",
       "      <td>7.751861e+06</td>\n",
       "      <td>111.615731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.769395e+04</td>\n",
       "      <td>1.211482</td>\n",
       "      <td>3857.539191</td>\n",
       "      <td>6.578409e+06</td>\n",
       "      <td>2661.018139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.100000e+01</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>-33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.535315e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.470396e+06</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.100000e+02</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.434041e+07</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.700375e+07</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>608956.000000</td>\n",
       "      <td>2.124802e+07</td>\n",
       "      <td>766035.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ratings_count  average_rating  text_reviews_count            id  \\\n",
       "count   3.517670e+05   351767.000000       351767.000000  3.517670e+05   \n",
       "mean    4.770586e+03        3.651194          330.993243  7.751861e+06   \n",
       "std     9.769395e+04        1.211482         3857.539191  6.578409e+06   \n",
       "min    -4.100000e+01      -31.000000            0.000000  4.000000e+00   \n",
       "25%     6.000000e+00        3.600000            1.000000  1.535315e+06   \n",
       "50%     3.900000e+01        3.950000            8.000000  6.470396e+06   \n",
       "75%     3.100000e+02        4.250000           53.000000  1.434041e+07   \n",
       "max     2.700375e+07        5.000000       608956.000000  2.124802e+07   \n",
       "\n",
       "          fans_count  \n",
       "count  351767.000000  \n",
       "mean      111.615731  \n",
       "std      2661.018139  \n",
       "min       -33.000000  \n",
       "25%         1.000000  \n",
       "50%         4.000000  \n",
       "75%        20.000000  \n",
       "max    766035.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104296\n",
      "129604\n",
      "253327\n",
      "253327\n",
      "264572\n",
      "264606\n",
      "264611\n",
      "264638\n",
      "264743\n",
      "264747\n",
      "264859\n",
      "264876\n",
      "264878\n",
      "264919\n",
      "265036\n",
      "265152\n",
      "270664\n",
      "274931\n",
      "274940\n",
      "275034\n",
      "275039\n",
      "275082\n",
      "282878\n",
      "285757\n",
      "286226\n",
      "337953\n",
      "338249\n",
      "343284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[104296,\n",
       " 129604,\n",
       " 253327,\n",
       " 253327,\n",
       " 264572,\n",
       " 264606,\n",
       " 264611,\n",
       " 264638,\n",
       " 264743,\n",
       " 264747,\n",
       " 264859,\n",
       " 264876,\n",
       " 264878,\n",
       " 264919,\n",
       " 265036,\n",
       " 265152,\n",
       " 270664,\n",
       " 274931,\n",
       " 274940,\n",
       " 275034,\n",
       " 275039,\n",
       " 275082,\n",
       " 282878,\n",
       " 285757,\n",
       " 286226,\n",
       " 337953,\n",
       " 338249,\n",
       " 343284]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(len(authors['average_rating'])):\n",
    "    if authors['average_rating'][i]<0:\n",
    "        print(i)\n",
    "        l.append(i)\n",
    "    if authors['ratings_count'][i]<0:\n",
    "        print(i) \n",
    "        l.append(i)\n",
    "    if authors['fans_count'][i]<0:\n",
    "        print(i)\n",
    "        l.append(i)\n",
    "l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275082,\n",
       " 253327,\n",
       " 286226,\n",
       " 264859,\n",
       " 264606,\n",
       " 337953,\n",
       " 264611,\n",
       " 264743,\n",
       " 264747,\n",
       " 264876,\n",
       " 264878,\n",
       " 264572,\n",
       " 285757,\n",
       " 264638,\n",
       " 265152,\n",
       " 129604,\n",
       " 270664,\n",
       " 338249,\n",
       " 265036,\n",
       " 264919,\n",
       " 275034,\n",
       " 275039,\n",
       " 104296,\n",
       " 274931,\n",
       " 343284,\n",
       " 274940,\n",
       " 282878]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(set(l))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>id</th>\n",
       "      <th>fans_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.517400e+05</td>\n",
       "      <td>351740.000000</td>\n",
       "      <td>351740.000000</td>\n",
       "      <td>3.517400e+05</td>\n",
       "      <td>351740.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.770952e+03</td>\n",
       "      <td>3.651283</td>\n",
       "      <td>331.018414</td>\n",
       "      <td>7.751783e+06</td>\n",
       "      <td>111.624663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.769769e+04</td>\n",
       "      <td>1.210085</td>\n",
       "      <td>3857.686173</td>\n",
       "      <td>6.578581e+06</td>\n",
       "      <td>2661.120073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.534690e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.470274e+06</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.100000e+02</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.434083e+07</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.700375e+07</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>608956.000000</td>\n",
       "      <td>2.124802e+07</td>\n",
       "      <td>766035.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ratings_count  average_rating  text_reviews_count            id  \\\n",
       "count   3.517400e+05   351740.000000       351740.000000  3.517400e+05   \n",
       "mean    4.770952e+03        3.651283          331.018414  7.751783e+06   \n",
       "std     9.769769e+04        1.210085         3857.686173  6.578581e+06   \n",
       "min     0.000000e+00        0.000000            0.000000  4.000000e+00   \n",
       "25%     6.000000e+00        3.600000            1.000000  1.534690e+06   \n",
       "50%     3.900000e+01        3.950000            8.000000  6.470274e+06   \n",
       "75%     3.100000e+02        4.250000           53.000000  1.434083e+07   \n",
       "max     2.700375e+07        5.000000       608956.000000  2.124802e+07   \n",
       "\n",
       "          fans_count  \n",
       "count  351740.000000  \n",
       "mean      111.624663  \n",
       "std      2661.120073  \n",
       "min         0.000000  \n",
       "25%         1.000000  \n",
       "50%         4.000000  \n",
       "75%        20.000000  \n",
       "max    766035.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in l:\n",
    "    authors= authors.drop(i)\n",
    "    \n",
    "authors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now there no 'strange' values and ratings_count, average_rating and fans_count have not any outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hist of the average ratings of all the authors\n",
    "import matplotlib.pyplot as plt \n",
    "plt.hist(authors.average_rating, bins=50, edgecolor='black')\n",
    "plt.xlim(0,5)\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the average rating')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are a lot of ratings with values from 4.0 to 5.0 and 0 and only a small part with rating values from 1.0 to 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'works_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\File\\GitHub\\ADM-HW2\\main.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW2/main.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#plot of the works_count of all the authors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW2/main.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW2/main.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mbar(authors\u001b[39m.\u001b[39mname, authors\u001b[39m.\u001b[39mworks_count, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mskyblue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW2/main.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#plt.xlim(0,5)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW2/main.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mRatings\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'works_count'"
     ]
    }
   ],
   "source": [
    "#plot of the works_count of all the authors\n",
    "import matplotlib.pyplot as plt \n",
    "plt.bar(authors.name, authors.works_count, color='skyblue')\n",
    "\n",
    "#plt.xlim(0,5)\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the average rating')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = authors[[\"average_rating\"]]\n",
    "Y = authors[[\"ratings_count\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "Y_standardized = scaler.fit_transform(Y)\n",
    "\n",
    "df_standardized = pd.DataFrame({\"X_standardized\": X_standardized.flatten(), \"Y_standardized\": Y_standardized.flatten()})\n",
    "\n",
    "plt.scatter(df_standardized[\"X_standardized\"], df_standardized[\"Y_standardized\"], color='blue', marker='o')\n",
    "\n",
    "plt.title('Plot: average_rating and ratings_count')\n",
    "plt.xlabel('average_rating')\n",
    "plt.ylabel('ratings_count')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(authors['average_rating'] ,authors['ratings_count'], color='blue', marker='o',alpha=0.5)#scatterplot of average ratings and rating count\n",
    "plt.show()\n",
    "print(authors['average_rating'].corr(authors['ratings_count']))  #correlation between average ratings and rating count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between average ratings and rating count is very close to 0, that indicates that there is no relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(authors.average_rating, authors.fans_count, color='red', marker='o', alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(authors['average_rating'].corr(authors['fans_count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between average ratings and fans count is very close to 0, that indicates that there is no relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = authors[[\"works_count\"]]\n",
    "Y = authors[[\"fans_count\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "Y_standardized = scaler.fit_transform(Y)\n",
    "\n",
    "df_standardized = pd.DataFrame({\"X_standardized\": X_standardized.flatten(), \"Y_standardized\": Y_standardized.flatten()})\n",
    "\n",
    "plt.scatter(df_standardized[\"X_standardized\"], df_standardized[\"Y_standardized\"], color='red', marker='o')\n",
    "\n",
    "plt.title('Plot: works_count and fans_count')\n",
    "plt.xlabel('works_count')\n",
    "plt.ylabel('fans_count')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "correlation = df_standardized['X_standardized'].corr(df_standardized['Y_standardized'])\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between works count and fans count is very close to 0, that indicates that there is no relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.language.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ2] Letâs finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:24:46.264000100Z",
     "start_time": "2023-10-31T16:24:46.223768700Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = books['author_name'].value_counts().sort_values(ascending=False).head(70)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(counts.index, counts.values, color='skyblue')\n",
    "plt.title('Number of Books per Author (Descending Order)')\n",
    "plt.xlabel('Author Name')\n",
    "plt.ylabel('Number of Books')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?\n",
    "\n",
    "---\n",
    "\n",
    "We simply do a query using the books dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:24:47.099936200Z",
     "start_time": "2023-10-31T16:24:46.230747600Z"
    }
   },
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:25:13.910696500Z",
     "start_time": "2023-10-31T16:24:50.854340200Z"
    }
   },
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:25:31.077161700Z",
     "start_time": "2023-10-31T16:25:21.076890300Z"
    }
   },
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the bookâs dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:25:31.081674500Z",
     "start_time": "2023-10-31T16:25:31.079669400Z"
    }
   },
   "outputs": [],
   "source": [
    "books.language.head(30)\n",
    "#there are some spazi vuoti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(books.language)):\n",
    "    if books.language[i]=='':\n",
    "        print(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many nan value so we decide to use a dataset without them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua = books[books[\"language\"] != \"\"]\n",
    "lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = lingua['language'].value_counts().head(20)\n",
    "plt.bar(languages.index, languages.values, color='skyblue')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graphic, a large part of the dataset is made up of books written in English (non specified if is US or CA or GB english) which is the 72.5% of all. Adding the US/GB/CA English the English books are the 83.5% of all the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(lingua['language'])\n",
    "\n",
    "perc_ing=32420/n*100\n",
    "perc_ing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?\n",
    "\n",
    "Notice that there are some entries that have a void string instead of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:25:37.281135500Z",
     "start_time": "2023-10-31T16:25:33.376159700Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a view that excludes the entries with void string\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "# execute query\n",
    "result = df[df[\"num_pages\"].astype(int) > 250].shape[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = authors.sort_values(by=\"works_count\", ascending=False).head(50)\n",
    "plt.bar(top_authors[\"name\"], top_authors[\"fans_count\"], color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some 'strange' values like NOT A BOOK or Unknown or Anonymous, let's plot without these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = authors[(authors[\"name\"] != \"NOT A BOOK\") & (authors[\"name\"] != \"Unknown\") & (authors[\"name\"] != \"Anonymous\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = df.sort_values(by=\"works_count\", ascending=False).head(50)\n",
    "plt.bar(top_authors[\"name\"], top_authors[\"fans_count\"], color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ3] Letâs have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.1:** Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "* The number of books published that year.\n",
    "* The total number of pages written that year.\n",
    "* The most prolific month of that year.\n",
    "* The longest book written that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:25:42.826421800Z",
     "start_time": "2023-10-31T16:25:42.810692100Z"
    }
   },
   "outputs": [],
   "source": [
    "# num pages cleaning\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "df[\"num_pages\"] = df[\"num_pages\"].astype(int)\n",
    "\n",
    "# date information gathering\n",
    "# TODO CHANGE ORIGINAL PUBLICATION DATE WITH PUBLICATION DATE\n",
    "pattern = r\"(-?[1-9][0-9]{3}|[1-9][0-9]{2}|[1-9][0-9]|[0-9])(?:-(0[1-9]|1[0-2]))?(?:-([0-2][0-9]|3[0-1]))?\"\n",
    "df[[\"year\", \"month\", \"day\"]] = df[\"original_publication_date\"].str.extract(pattern)\n",
    "df = df[~df[\"year\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to ignore every book that has an invalid date format. We decided to also treat those years that have negative date (meaning BC), however for those dates (but not only for those) no month is given. Also some years for which month are used they are not always present. When they are present we for the calculation about the most prolific month we only use those books who specify the month, while when no month is specified we use the placeholder value \"00\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_by_year(books, year):\n",
    "    books_year = books[books[\"year\"] == str(year)]\n",
    "\n",
    "    n_books = books_year.shape[0]\n",
    "    tot_pages = books_year[\"num_pages\"].sum()\n",
    "    longest_book = books_year[books_year[\"num_pages\"] >= books_year[\"num_pages\"].max()][\"title\"].head(1).item()\n",
    "\n",
    "    try:\n",
    "        prolific_month = books_year.groupby(\"month\")[\"num_pages\"].sum().idxmax()\n",
    "    except ValueError as e:\n",
    "        prolific_month = \"00\"\n",
    "\n",
    "    return n_books, tot_pages, prolific_month, longest_book\n",
    "\n",
    "# test the function with a random year\n",
    "\n",
    "look_by_year(df, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.2:** Use this function to build your data frame: the primary key will be a year, and the required information will be the attributes within the row. Finally, show the head and the tail of this new data frame considering the first ten years registered and the last ten years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_year_dataset = pd.DataFrame()\n",
    "\n",
    "for year in tqdm(df[\"year\"].unique(), total = df[\"year\"].unique().shape[0]):\n",
    "    per_year_dataset = pd.concat([per_year_dataset, pd.DataFrame((year,) + look_by_year(df, year)).T])\n",
    "\n",
    "per_year_dataset = per_year_dataset.rename(columns = {0: \"year\", 1: \"num_books\", 2: \"tot_pages\", 3: \"prolific_month\", 4: \"longest_book\"})\n",
    "per_year_dataset = per_year_dataset.set_index(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_year_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_year_dataset.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.3:** Ask ChatGPT or any other LLM chatbot tool to implement this function and compare your work with the one the bot gave you as an answer. Does the chatbot implementation work? Please test it out and verify the correctness of the implementation, explaining the process you followed to prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.1:** You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T14:32:19.482069300Z",
     "start_time": "2023-11-02T14:32:17.527174Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check whether the entry is a duplicate or a different author\n",
    "\n",
    "names = authors['name'].value_counts()\n",
    "dupl_auth = names[names > 1]\n",
    "\n",
    "print(\"Number of double entries: \", len(dupl_auth))\n",
    "\n",
    "eponymous = []\n",
    "duplicates = []\n",
    "\n",
    "# iterate through the authors dataframe names, and if the same name matches 2 ids, append to eponymous, otherwise if singular id for both entries, append to duplicates\n",
    "\n",
    "for i in range(len(dupl_auth.index)):\n",
    "    auth_df = authors[authors['name'] == dupl_auth.index[i]]\n",
    "    if len(list(set(auth_df['book_ids'].iloc[0] + auth_df['book_ids'].iloc[1]))) == len(auth_df['book_ids'].iloc[0]) + len(auth_df['book_ids'].iloc[1]):\n",
    "        eponymous.append(dupl_auth.index[i])\n",
    "    else:\n",
    "        duplicates.append(dupl_auth.index[i])\n",
    "    \n",
    "print(\"The duplicate entries referring to the same authors are:\\n\", duplicates)\n",
    "print(\"\\n The actually eponymous authors are:\\n\", eponymous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.2:** Write a function that, given a list of author_id, outputs a dictionary where each author_id is a key, and the related value is a list with the names of all the books the author has written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:29:41.657526600Z",
     "start_time": "2023-11-02T16:29:41.628008600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def author_works_dic(author_ids: list):\n",
    "    auth_books = {i: None for i in author_ids}\n",
    "    for i in author_ids:\n",
    "        try:\n",
    "            bks = authors[authors['id'] == i].iloc[0][4] \n",
    "            ttls = list()\n",
    "            for bk in bks:                   \n",
    "                if books[books['id'] == int(bk)].shape[0] != 0: \n",
    "                    ttls.append(books[books['id'] == int(bk)].iloc[0]['title']) \n",
    "            auth_books[i] = ttls \n",
    "        except IndexError:\n",
    "            print(\"No author with ID: \", i)\n",
    "    return auth_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.3:** What is the longest book title among the books of the top 20 authors regarding their average rating? Is it the longest book title overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:44:33.092133900Z",
     "start_time": "2023-11-02T16:43:08.772331600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_20_rating_auth = authors.nlargest(10, \"average_rating\")[\"id\"]\n",
    "top_20_rating_books = author_works_dic(top_20_rating_auth)\n",
    "\n",
    "longest_title = \"\"\n",
    "\n",
    "for i in top_20_rating_books.values():\n",
    "    for title in i:\n",
    "        if len(title) > len(longest_title):\n",
    "            longest_title = title\n",
    "\n",
    "longest_overall_title = books['title'].str.len().idxmax()\n",
    "print(\"The longest title is:\", longest_title)\n",
    "print(\"\\n The longest overall title is:\", longest_overall_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.4:** What is the shortest overall book title in the dataset? If you find something strange, provide a comment on what happened and an alternative answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T16:46:00.261513500Z",
     "start_time": "2023-11-02T16:45:43.308888Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not books[\"title\"].isnull():\n",
    "    shortest_book_ever = books[\"title\"].str.len().idmin()\n",
    "\n",
    "print(shortest_book_ever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ5] We can consider the authors with the most fans to be influential. Letâs have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.1a:** Plot the top 10 most influential authors regarding their fan count and number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:49:54.474245800Z",
     "start_time": "2023-10-31T16:49:46.746746Z"
    }
   },
   "outputs": [],
   "source": [
    "# count the actual number of books written by every author using the length of the list of his book ids.\n",
    "authors[\"book_count\"] = authors[\"book_ids\"].apply(len)\n",
    "\n",
    "# 10 most influential authors regarding their fan count\n",
    "x = authors.nlargest(10, \"fans_count\")\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(x[\"name\"], x[\"fans_count\"], color = 'green', label = 'Fan Count')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(x[\"name\"], x[\"book_count\"], color = 'blue', label = 'Book Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.1b:** Plot the top 10 most influential authors regarding their fan count and number of books. Who is the most influential author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:49:54.691100300Z",
     "start_time": "2023-10-31T16:49:54.453201300Z"
    }
   },
   "outputs": [],
   "source": [
    "# most influential author\n",
    "m_inf = authors.nlargest(1, \"fans_count\")\n",
    "print(\"The most influential author, having by far the largest fanbase is:\", m_inf[\"name\"].to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.2:** Have they published any series of books? If any, extract the longest series name among these authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:50:24.987942600Z",
     "start_time": "2023-10-31T16:50:18.020833300Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a new dataframe containing only the books written by the top 10 authors\n",
    "top_authors_books = books[books[\"author_id\"].isin(list(x[\"id\"]))]\n",
    "\n",
    "# get the series authors by their \"series position\" to find who has written series, and use max position to determine their longest series' length.\n",
    "author_series_lengths = top_authors_books.groupby([\"author_id\", \"author_name\"])[\"series_position\"].max().reset_index()\n",
    "\n",
    "# return the highest series position, thus the longest series author and name\n",
    "author_series = top_authors_books.groupby([\"author_id\", \"author_name\", \"series_name\"])[\"series_position\"].max().reset_index()\n",
    "max_series = author_series[author_series[\"series_position\"] == author_series[\"series_position\"].max()]\n",
    "\n",
    "print(\"Authors of series are:\")\n",
    "print(author_series_lengths[\"author_name\"].to_string(index = False))\n",
    "\n",
    "print(\"\\nThe author of the longest series is:\")\n",
    "print(max_series[\"author_name\"].to_string(index = False))\n",
    "print(\"And the series is:\")\n",
    "print(max_series[\"series_name\"].to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.3:** How many of these authors have been published in different formats? Provide a meaningful chart on the distribution of the formats and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:50:50.051894400Z",
     "start_time": "2023-10-31T16:50:47.793254400Z"
    }
   },
   "outputs": [],
   "source": [
    "# to find the authors published in different formats from the top 10\n",
    "authors_with_different_formats = top_authors_books[top_authors_books.duplicated(subset=[\"author_id\"], keep = False)]\n",
    "authors_with_different_formats = authors_with_different_formats[authors_with_different_formats[\"format\"] != \"\"]\n",
    "\n",
    "# to avoid name repetition\n",
    "unique_author_ids = authors_with_different_formats[\"author_id\"].unique()\n",
    "\n",
    "# create a separate chart for each author with multiple formats\n",
    "for author_id in unique_author_ids:\n",
    "    author_data = authors_with_different_formats[authors_with_different_formats[\"author_id\"] == author_id]\n",
    "    author_name = author_data[\"author_name\"].values[0]\n",
    "    format_distribution = author_data[\"format\"].value_counts()\n",
    "    \n",
    "    format_distribution.plot(kind='bar')\n",
    "    plt.title(f\"Formats Distribution for {author_name}\")\n",
    "    plt.xlabel(\"Format\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on the graphs above:** we can see that the books are mainly published in paperback and hardcover formats, whereas the \"newer\" digital formats are very few. This would be due to 3 main reasons: \n",
    "1- the age of most of the books written by these authors since audiobooks and ebook formats are relatively new.\n",
    "2- even with more widely available digital formats now, these are authors of novels and readers who usually enjoy leisurely reading prefer the physical format of paper, rather than reading through a screen.\n",
    "3- Once a book has been published in a digital format, there is no need to republish it using another editorial, since it is already on the internet, a more universal \"bookstore\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.4:** Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender, and comment about anything eventually related to âstructural bias.â You may want to have a look at the following recommended readings:\n",
    "\n",
    "- https://bhm.scholasticahq.com/article/38021\n",
    "- https://priyanka-ddit.medium.com/how-to-deal-with-imbalanced-dataset-86de86c49\n",
    "- https://compass.onlinelibrary.wiley.com/doi/10.1111/soc4.12962\n",
    "\n",
    "You can even ask ChatGPT or any other LLM chatbot tool: try to formulate a prompt that provides helpful information about it. Put that information in your notebook and provide comments on what you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T14:07:56.622333100Z",
     "start_time": "2023-11-02T14:07:55.437387400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform gender values to \"male,\" \"female,\" or \"other\"\n",
    "authors['gender'] = authors['gender'].str.lower().apply(\n",
    "    lambda x: 'male' if x == 'male' else ('female' if x == 'female' else 'other'))\n",
    "\n",
    "# Group the new simpler data by gender, and get mean values\n",
    "gender_grouped = authors.groupby('gender').agg({'average_rating': 'mean', 'fans_count': 'mean'}).reset_index()\n",
    "\n",
    "# Plots to visibly show any disparities\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot the first subplot for average ratings\n",
    "ax1.bar(gender_grouped['gender'], gender_grouped['average_rating'])\n",
    "ax1.set_title('Average Ratings by Gender')\n",
    "ax1.set_xlabel('Gender')\n",
    "ax1.set_ylabel('Mean Average Rating')\n",
    "\n",
    "# Plot the second subplot for fan counts\n",
    "ax2.bar(gender_grouped['gender'], gender_grouped['fans_count'])\n",
    "ax2.set_title('Fan Counts by Gender')\n",
    "ax2.set_xlabel('Gender')\n",
    "ax2.set_ylabel('Mean Fan Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Analysis:**\n",
    "\n",
    "Average Ratings:\n",
    "\n",
    "In terms of average rating, males, females and unassigned authors do not witness any visible disparity, all being relatively close to each other with only a slight advantage for females.\n",
    "\n",
    "Fan Count:\n",
    "\n",
    "The fan count is another story however where females have a far higher fan count than both males and \"other\", at almost double the men's and quadruple the \"others\". \n",
    "\n",
    "Please note now however that this analysis still holds regardless of the \"heavy\" cleaning and grouping of genders since even if some of the \"grouped values\" like he/him for example would actually be grouped into the \"male\" section. The reason why the distribution would still be similar under the assumption that the \"falsely grouped\" values would be evenly spread into both \"male and female\", where the only final result that would meaningfully change is the number of fans to the authors who really fall into the \"others\" group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?\n",
    "    For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 6.1:** Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:58:21.669045900Z",
     "start_time": "2023-10-31T16:58:21.175679800Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 1: transform the publication date entry to datetime format\n",
    "top_authors_books[\"original_publication_date\"] = pd.to_datetime(top_authors_books[\"original_publication_date\"], format = 'mixed')\n",
    "\n",
    "# calculate time gap between consecutive \"original\" publications and add it into a column for all books\n",
    "top_authors_books[\"time_gap\"] = top_authors_books.groupby([\"author_id\", \"series_id\"])[\"original_publication_date\"].diff()\n",
    "\n",
    "# calculate the average time gap for books in a series and not in a series, according to the new column created\n",
    "average_time_gap_series = top_authors_books[top_authors_books[\"series_id\"].notna()][\"time_gap\"].mean()\n",
    "average_time_gap_non_series = top_authors_books[top_authors_books[\"series_id\"].isna()][\"time_gap\"].mean()\n",
    "\n",
    "# print(f\"Average Time Gap for Books in a Series: {average_time_gap_series}\")\n",
    "# print(f\"Average Time Gap for Books Not in a Series: {average_time_gap_non_series}\")\n",
    "print(top_authors_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we expect to see?** As expected the books that belong to a series have a more periodic and regular output from the authors(on average), and this logically makes sense following the idea that the author is breaking down a full story into separate books.\n",
    "Books that do not belong to a series however do not have a well-defined average gap between them since they are independent works and do not follow any periodic output \"expectation\" from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 6.2:** For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:58:11.052616700Z",
     "start_time": "2023-10-31T16:58:09.593081400Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a new column using the datetime format of the original publication date\n",
    "top_authors_books['year'] = top_authors_books[\"original_publication_date\"].dt.year\n",
    "writers_counts = top_authors_books.groupby(['author_name', 'year']).size().reset_index(name='Count')\n",
    "\n",
    "# plot the publication history for each author\n",
    "writers = writers_counts['author_name'].unique()\n",
    "for writer in writers:\n",
    "    author_data = writers_counts[writers_counts['author_name'] == writer]\n",
    "    plt.plot(author_data['year'], author_data['Count'], label=writer)\n",
    "\n",
    "plt.title(\"Publication History of Authors\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Books Published\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.1:** Estimate the probability that a book has over 30% of the ratings above 4.\n",
    "\n",
    "---\n",
    "\n",
    "We can do this by counting how many books have 30% of the rating above 4 (i.e. a rating of 5 since you can only give an integer beteween 1 and 5 as rating) and dividing it by the total number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"rating_dist\"] == \"5:-1|4:1|3:1|2:0|1:0|total:1\"]\n",
    "books[books[\"rating_dist\"] == \"5:0|4:0|3:3|2:-1|1:-1|total:1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on slack they said you should get also 4 scores\n",
    "\n",
    "def has_30p_above_four(rating_dist):\n",
    "    pattern = r\"5:([0-9]+)\\|4:[0-9]+\\|3:[0-9]+\\|2:[0-9]+\\|1:[0-9]+\\|total:([0-9]+)\"\n",
    "\n",
    "    match = re.search(pattern, rating_dist)\n",
    "    if int(match.group(1)) > 0.3 * int(match.group(2)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df = books.drop(index = [6650514, 7011288]) # we remove this specific cases because the rating distribution is wrong\n",
    "above_four = sum(df[\"rating_dist\"].apply(has_30p_above_four))\n",
    "\n",
    "result = above_four / df.shape[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.2:** Estimate the probability that an author publishes a new book within two years from its last work.\n",
    "\n",
    "---\n",
    "\n",
    "Not every author on the authors dataset has a book in the books dataset, that is because our datasets are only a subset of the original one. This means that we there could be come authors that have published a new book within two years from their last work but since that book istance is not present in our dataset and we do not count it. However we could not even say that our estimate is a lower bound of the real probability because not every author is present in our dataset.\n",
    "\n",
    "We divide by the number of books minus the number of authors to obtain the total number of \"non-first books\".\n",
    "\n",
    "TODO Dobbiamo contare quanti autori lo hanno fatto almeno una volta o quanti libri hanno questa proprietÃ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_7468\\1005961742.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"month\"] = df[\"month\"].fillna(0).astype(int) + df[\"year\"].astype(int) * 12\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_7468\\1005961742.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop([\"year\", \"day\"], axis = 1, inplace = True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a5fb1ed522400fa28a5bad84c998cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an author publishes a new book whitin two years from it's last work is:  0.17181312350820072\n"
     ]
    }
   ],
   "source": [
    "df = books\n",
    "pattern = r\"(-?[1-9][0-9]{3}|[1-9][0-9]{2}|[1-9][0-9]|[0-9])(?:-(0[1-9]|1[0-2]))?(?:-([0-2][0-9]|3[0-1]))?\"\n",
    "df[[\"year\", \"month\", \"day\"]] = df[\"original_publication_date\"].str.extract(pattern)\n",
    "df = df[(~df[\"year\"].isna())]\n",
    "df[\"month\"] = df[\"month\"].fillna(0).astype(int) + df[\"year\"].astype(int) * 12\n",
    "df.drop([\"year\", \"day\"], axis = 1, inplace = True)\n",
    "\n",
    "grouped = df.groupby(\"author_id\")\n",
    "\n",
    "score = 0\n",
    "for group_name, group_data in tqdm(grouped, total = grouped.ngroups):\n",
    "    group_data = group_data.sort_values(by = \"month\")\n",
    "    previous = group_data[\"month\"].head(1).item()\n",
    "\n",
    "    for elem in group_data[\"month\"]:\n",
    "        if elem - previous != 0 and elem - previous <= 48:\n",
    "            score += 1\n",
    "\n",
    "result = score / (df.shape[0] - df[\"author_id\"].unique().shape[0])\n",
    "print(\"The probability that an author publishes a new book whitin two years from it's last work is: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.3:** In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\" Estimate the probability of a book being included in this list, knowing it has more than 700 pages.\n",
    "\n",
    "---\n",
    "\n",
    "We call the \"Book is in the worst books list\" event $A$ and \"Book has more than 700 pages\" event $B$. We use Bayes' formula:\n",
    "\n",
    "$\\mathbb{P}(\\text{A | B}) = \\frac{\\mathbb{P}(\\text{B | A}) \\;\\cdot\\; \\mathbb{P}(\\text{A}) }{\\mathbb{P}(\\text{B})}$\n",
    "\n",
    "* $\\mathbb{P}(\\text{A})$: the probability that a book is in the worst books list. Obtained by counting the entries in \"The Worst Books of All Time\" list and dividing it by the number of books in the dataset.\n",
    "* $\\mathbb{P}(\\text{B})$: the probability that a book has more than 700 pages. Obtained by counting the entries with more than 700 pages and dividing it by the number of books in the dataset.\n",
    "* $\\mathbb{P}(\\text{B | A})$: the probability of a book having more than 700 pages knowing that it is in the worst book list. Obtained by doing the second query over the books identified by first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "worst_books = pd.read_json(list_path, lines = True, nrows = 1)[\"books\"][0]\n",
    "worst_books = pd.DataFrame(worst_books)\n",
    "worst_books[\"book_id\"] = worst_books[\"book_id\"].astype(int)\n",
    "n_worst_books = worst_books.shape[0]\n",
    "join = pd.merge(worst_books, df, left_on = \"book_id\", right_on = \"id\", how = \"inner\", suffixes = [\"_w\", \"_b\"])\n",
    "\n",
    "pA = n_worst_books / df.shape[0]\n",
    "pB = df[df[\"num_pages\"].astype(int) > 700].shape[0] / (df.shape[0])\n",
    "pBA = join[join[\"num_pages\"].astype(int) > 700].shape[0] / (join.shape[0])\n",
    "pAB = pBA * pA / pB\n",
    "\n",
    "print(\"P(A | B), the probability of a book being included in the worst books list, knowing it has more than 700 pages: \", pAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.4:** Are the events X=âBeing Included in The Worst Books of All Time listâ and Y=âHaving more than 700 pagesâ independent? Explain how you have obtained your answer.\n",
    "\n",
    "---\n",
    "\n",
    "To check whether the X and Y are indipendent we can apply the definition of indipendence. Two events are indipendent if knowing that one happened does not change the probability of the second one appearing. Formally speaking, that is when:\n",
    "\n",
    "$\\mathbb{P}(X \\cap Y) = \\mathbb{P}(X) \\cdot \\mathbb{P}(Y)$\n",
    "\n",
    "To reuse the previously calculated probabilities we can also add the definition of conditional probability:\n",
    "\n",
    "$\\mathbb{P}(X | Y) = \\frac{\\mathbb{P}(X \\cap Y)}{\\mathbb{P}(Y)}$\n",
    "\n",
    "From which we obtain that if X and Y are indipendent:\n",
    "\n",
    "$\\mathbb{P}(X | Y) = \\mathbb{P}(X)$\n",
    "\n",
    "$\\mathbb{P}(Y | X) = \\mathbb{P}(Y)$\n",
    "\n",
    "\n",
    "We called X as A and Y as B. Then to check for indipendence we can just check whether $\\mathbb{P}(\\text{A | B})$ and $\\mathbb{P}(\\text{B | A})$ are equal respectively to $\\mathbb{P}(A)$ and $\\mathbb{P}(B)$. To be precise indipendence is a two-way relation, so if one holds also the second must hold, this means we really only need to check one of those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"in_worst_book_list\"] = df[\"id\"].apply(lambda x: x in worst_books[\"book_id\"].values)\n",
    "# df[\"more_700_pages\"] = df[\"num_pages\"].apply(lambda x: x > 700)\n",
    "\n",
    "print(\"Test 1:\")\n",
    "print(\"P(A), the probability of a book of the whole dataset to be in the worst books list: \", pA)\n",
    "print(\"P(A | B), the probability of a book to be in the worst books list, knowing that it has more than 700 pages: \", pAB)\n",
    "\n",
    "print(\"Test 2:\")\n",
    "print(\"P(B), the probability of book of the whole dataset to have more than 700 pages over the total: \", pB)\n",
    "print(\"P(B | A), the probability of books to have more than 700 pages knowing that it is in the worst books list: \", pBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the two events are dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.1:** Can you demonstrate that readers usually rate the longest books as the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "df[\"num_pages\"] = df.num_pages.astype(int)\n",
    "\n",
    "# we try to remove outliers\n",
    "df = df[(df[\"num_pages\"] < 1500) & (df[\"num_pages\"] != 0) & (df[\"average_rating\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"num_pages\"], df[\"average_rating\"])\n",
    "\n",
    "plt.ticklabel_format(axis = \"y\", style = \"plain\")\n",
    "plt.xlabel(\"Num pages\")\n",
    "plt.ylabel(\"Average rating\")\n",
    "\n",
    "z = np.polyfit(df[\"num_pages\"],  df[\"average_rating\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"num_pages\"], p(df[\"num_pages\"]), color= \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatterplot is confusing, we try to use a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(df[\"num_pages\"], df[\"average_rating\"], gridsize = 20, bins = \"log\", edgecolor = \"gray\")\n",
    "\n",
    "z = np.polyfit(df[\"num_pages\"],  df[\"average_rating\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"num_pages\"], p(df[\"num_pages\"]), color= \"black\")\n",
    "\n",
    "plt.xlabel(\"Num pages\")\n",
    "plt.ylabel(\"Average rating\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df[\"average_rating\"].corr(df[\"num_pages\"])\n",
    "print(\"The correlation coefficient between 'average_rating' and 'num_pages' is:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the correlation coefficient is and in the positive direction, hinting at a faint positive correlation between the number of pages and the average rating. Without our data selection the correlation is even fainter but always in a positive direction. This means that we can't demonstrate that readers usually rate the longest books as the worst, instead even if the effect is very little it's the opposite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.2:** Compare the average rate distribution for English and non-English books with a proper statistical procedure. What can you conclude about those two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the books into 2 groups\n",
    "eng_books = lingua[(lingua['language'] == 'eng') | (lingua['language'] == 'en-US') | (lingua['language'] == 'en-GB') | (lingua['language'] == 'en-CA')]\n",
    "non_eng_books = lingua[(lingua['language'] != 'eng') & (lingua['language'] != 'en-US')& (lingua['language'] != 'en-GB') & (lingua['language'] != 'en-CA')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's check the histograms of the average ratings of each groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(eng_books.average_rating, bins=50, edgecolor='black')\n",
    "plt.xlim(0,5)\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the average rating for english books')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(non_eng_books.average_rating, bins=50, edgecolor='black')\n",
    "plt.xlim(0,5)\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the average rating for non english books')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms it looks like there are no differences of ratings if the book is written in english or not. The two distributions are similar to a normal distribution.\n",
    "Let's see if this hypothesis is true or not doing some test. I choose to do a t-test which is a parametric test for determining correlations between two samples of data.\n",
    "\n",
    "So we define:\n",
    "* The null hypothesis H0 is: there no difference between the 2 groups in terms of average rating.\n",
    "* The alternative hypothesis H1 is: there is a difference between the 2 groups in terms of average rating.\n",
    "\n",
    "We also need to choose an alpha, usually is 0.05, which means that the probability to say that they are different when they are not is not more than 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(eng_books.average_rating))\n",
    "print(np.var(non_eng_books.average_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have different variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "alpha = 0.05\n",
    "t_statistic, p_value = ttest_ind(eng_books['average_rating'], non_eng_books['average_rating'], equal_var=False)\n",
    "\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"t-statistic:\", t_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting p-value is smaller than alpha, so we reject the null hypothesis, this means that there is a significant difference between the distribution of the average rating of the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.3:** About the two groups in the previous question, extract helpful statistics like mode, mean, median, and quartiles, explaining their role in a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_books.average_rating.describe())\n",
    "print(\"The mode is:\", statistics.mode(eng_books.average_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_eng_books.average_rating.describe())\n",
    "print(\"The mode is:\", statistics.mode(non_eng_books.average_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "ax = fig.add_axes([0, 0, 0.7, 0.6])\n",
    "libri=[eng_books.average_rating, non_eng_books.average_rating]\n",
    "box = ax.boxplot(libri, widths=0.5) \n",
    "\n",
    "ax.set_xticklabels(['english books','non english books'])\n",
    "plt.title('Boxplot of average rating') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a boxplot the median is represented by the yellow line, and the first and third quintile are represented by the upper and lower line of the box.\\\n",
    "So for example: \\\n",
    "For the english books the median is 3.96 this means that 50% of the values are lower than 3.96 and if we see the distance of the first and third quintile from the median, we can notice that they are quite similar so this means they are simmetrical.\\\n",
    "The mode helps to identify around which value the distribution is centered in this case for the two groups is 4 and as we can see from the boxplots the distributions are centered in this value.\\\n",
    "Mean is not visible in the boxplot and for each group median and mean are softly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.4:** It seems reasonable to assume that authors with more fans should have more reviews, but maybe their fans are a bit lazy. Confirm or reject this with a convenient statistical test or a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = authors[authors[\"fans_count\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption is that fans of popular authors are more \"lazy\" and leave less reviews. We can check if there is a correlation between the numbers of fans and number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"fans_count\"], df[\"ratings_count\"])\n",
    "\n",
    "plt.ticklabel_format(axis = \"y\", style = \"plain\")\n",
    "plt.xlabel(\"Fans count\")\n",
    "plt.ylabel(\"Ratings per fan\")\n",
    "\n",
    "z = np.polyfit(df[\"fans_count\"],  df[\"ratings_count\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"fans_count\"], p(df[\"fans_count\"]), color= \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a correlation between the two. However a more precise measure would be to check for every author it's factor of \"review per fan\", that better shows the fact that fans of great authors are more lazy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"fans_count\"], df[\"ratings_count\"] / df[\"fans_count\"])\n",
    "\n",
    "plt.ticklabel_format(axis = \"y\", style = \"plain\")\n",
    "plt.xlabel(\"Fans count\")\n",
    "plt.ylabel(\"Ratings per fan\")\n",
    "\n",
    "z = np.polyfit(df[\"fans_count\"],  df[\"ratings_count\"] / df[\"fans_count\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"fans_count\"], p(df[\"fans_count\"]), color= \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers render the scatterplot difficult to read. Let's see something about this strange values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = df[(df[\"ratings_count\"] / df[\"fans_count\"]) == (df[\"ratings_count\"] / df[\"fans_count\"]).max()]\n",
    "outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided book dataset there is no information about what book this author wrote, but searching online we can see she is written as the editor of some of the most important Shakespeare plays, like Romeo and Juliet. This means that the column \"rating_count\" counts not only those books for which the person is the main author but everything that author has worked on. This also explains why she has a very high ratings count while having a very low fan count.\n",
    "\n",
    "We can try to remove this kind of authors from our dataset by setting a minimum threshold for \"fan_count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = authors[authors[\"fans_count\"] > 2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"fans_count\"], df[\"ratings_count\"] / df[\"fans_count\"])\n",
    "\n",
    "plt.ticklabel_format(axis = \"y\", style = \"plain\")\n",
    "plt.xlabel(\"Fans count\")\n",
    "plt.ylabel(\"Ratings per fan\")\n",
    "\n",
    "z = np.polyfit(df[\"fans_count\"],  df[\"ratings_count\"] / df[\"fans_count\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"fans_count\"], p(df[\"fans_count\"]), color= \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before we also plot a heatmap to better understand the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(df[\"fans_count\"], df[\"ratings_count\"] / df[\"fans_count\"], gridsize = 20, bins = \"log\", edgecolor = \"gray\")\n",
    "\n",
    "z = np.polyfit(df[\"fans_count\"],  df[\"ratings_count\"] / df[\"fans_count\"], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df[\"fans_count\"], p(df[\"fans_count\"]), color= \"black\")\n",
    "\n",
    "plt.xlabel(\"Fans count\")\n",
    "plt.ylabel(\"Ratings per fan\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we can say that the assumption made was correct, but the correlation is not very strong. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.5:** Provide a short survey about helpful statistical tests in data analysis and mining: focus on hypothesis design and the difference between parametric and nonparametric tests, explaining the reasons behind the choice of one of these two tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.1:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload authors.json dataset, and filter authors with at least 100 reviews. Do the same using Pandas and compare performance in terms of milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY ON MAIN PC\n",
    "\n",
    "# IN PANDAS\n",
    "\n",
    "print(\"Measuring Pandas performance...\")\n",
    "%timeit authors[authors[\"text_reviews_count\"] >= 100]\n",
    "\n",
    "# IN DASK\n",
    "\n",
    "authors_dask = dd.from_pandas(authors, npartitions = 1)\n",
    "\n",
    "print(\"Measuring Dask performance...\")\n",
    "%timeit authors_dask[authors_dask[\"text_reviews_count\"] >= 100].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a library very similar to Pandas but specialized to work on bigger datasets. It's greatest characteristic is intended parallel computing, intended both as local, where different cores of the same CPU are used at the same time, and cloud, where the computation is divided between computers clusters. Another important difference is it's \"lazy evalutaion\", operations are not executed when requested in the code but instead they are delayed and really executed only when explicitly told so or when necessart (Ex. if we request to plot the data it's necessary for the computation to be done).\n",
    "\n",
    "\n",
    "TODO\n",
    "We can't observe a meaningful difference between the two operations because the dataset is small and the machine this is executed on...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.2:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload books.json, and join them with authors.json based on author_id. How many books donât have a match for the author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "join = pd.merge(books, authors, left_on = \"author_id\", right_on = \"id\", how = \"left\")[[\"id_y\"]]\n",
    "authorless_istances = join[\"id_y\"].isna().sum()\n",
    "print(\"Authorless istances: \", authorless_istances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = dd.merge(dd.from_pandas(books, npartitions = 1), dd.from_pandas(authors, npartitions = 1), left_on = \"author_id\", right_on = \"id\", how = \"left\")\n",
    "authorless_instances = join[\"id_y\"].isna().sum().compute()\n",
    "print(\"Authorless instances: \", authorless_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Every book should have a field named description, and any author should have a field named description. Choose one of the two and perform a text-mining analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the text-mining analysis we created a list of tag words for specific literary genres that should help us identify books and and authors. List can have words in common as same concepts can appear in different genres (as \"monster\" can appear both in horror and fantasy genres) or when the same word is used with different meanings (as \"chemistry\" can appear both in romance and scientific books). Some words have spaces before and after to avoid finding it inside other words (as \"art\" in \"artificial\") or only before when we want to find also composed word (as for \"dread\" and \"dreadful\").\n",
    "\n",
    "We decided to use the same method for both books and authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_tags = [\"humor\", \"comedy\", \"satire\", \"funny\", \"laugh\", \"comical\", \"hilarious\", \"laughter\", \"light-hearted\", \"parody\", \"joke\", \"amusing\", \"whimsical\", \"lighthearted\", \"satirical\", \"witty\", \"silly\", \"absurd\", \"hilarity\", \"comedic\", \"farce\", \"irony\", \"sarcasm\", \"slapstick\"]\n",
    "action_tags = [\"action\", \"adventure\", \"suspense\", \"thriller\", \"bravery\", \"excitement\", \"adrenaline\", \"tension\", \"intense\", \"escape\", \"dangerous\", \"fast-paced\", \"adversary\", \"hero\", \"spy\", \"villain\", \"combat\", \"fight\", \"chase\", \"explosion\", \"fighting\", \"danger\", \"martial art\", \"high-octane\", \"adrenaline\", \"rescue\", \"mission\", \"conspiracy\", \"intelligence\", \"agent\", \"secret\"]\n",
    "fantasy_tags = [\"fantasy\", \"magic\", \"wizardry\", \"dragon\", \"god\", \"elf\", \"dwarf\", \"monster\", \"sword\", \"quest\", \"castle\", \"mythical\", \"prophecy\", \"fairy\", \"sorcery\", \"sorcerer\", \"witchcraft\", \"enchantment\", \"magician\", \"spellcasting\", \"conjuring\", \"curse\", \"alchemy\", \"mysticism\", \"occult\", \"supernatural\", \"sorcerer\", \"witch\", \"spell\", \"ritual\", \"arcane\", \"knowledge\", \"magical\", \"wizard\"]\n",
    "romance_tags = [\"romance\", \"love \", \"relationship\", \"passion\", \"heartfelt\", \"heart\", \"attraction\", \"emotional\", \"romantic\", \"couple\", \"affection\", \"intimacy\", \"sweetheart\", \"desire\", \"chemistry\", \"flirting\", \"courtship\", \"kiss\", \"embrace\", \"lover\", \"seduction\", \"infatuation\", \"lovesick\"]\n",
    "mystery_tags = [\"mystery\", \"detective\", \"suspense\", \"crime\", \"secrets\", \"clue\", \"mysterious\", \"victim\", \"suspicious\", \"suspicion\", \"death\", \"poison\", \"intrigue\", \"theft\", \"thief\", \"whodunit\", \"widow\", \"missing\", \"killer\", \"unsolved\", \"assassin\", \"twists\", \"investigation\", \"thriller\", \"clues\", \"sleuth\", \"police\", \"intrigue\", \"enigma\", \"puzzle\", \"murder\", \"conspiracy\", \"forensic\", \"stabbed\", \"solving\", \"sleuthing\", \"suspicion\", \"unraveling\", \"evidence\", \"schemes\", \"riddle\", \"murder\", \"revenge\", \"blackmail\", \"investigate\"]\n",
    "sci_fi_tags = [\"sci-fi\", \"planet\", \"science fiction\", \"space\", \"technology\", \"extraterrestrial\", \"future\", \"dystopian\", \"cyberpunk\", \"aliens\", \"time travel\", \"robots\", \"intergalactic\", \"virtual reality\", \"apocalypse\", \"AI\", \"nanotechnology\", \"post-apocalyptic\"]\n",
    "historical_tags = [\"historical\", \"history\", \"period\", \"accuracy\", \"context\", \" era \", \"research\", \"centuries\", \"century\", \"europe\", \"america\", \"asia\", \"africa\"]\n",
    "horror_tags = [\"horror\", \"scary\", \"terror\", \"fear\", \"creepy\", \" dread\", \"spooky\", \"haunting\", \"macabre\", \"dark\", \"nightmare\", \"ghost\", \"possessed\", \"demon\", \"paranormal\", \"frightening\", \"sinister\", \"psychological\", \"halloween\", \"chill\", \"gore\", \"haunted\"]\n",
    "science_tags = [\"science\", \"scientist\", \"research\", \"engineering\", \"discovery\", \"innovation\", \"technology\", \"scientific method\", \"experiment\", \"data\", \"analysis\", \"laboratory\", \"biology\", \"physic\", \"chemistry\", \"astronomy\", \"environment\", \"ecology\", \"genetic\", \"geology\", \"cosmology\", \"neuroscience\"]\n",
    "philosophy_tags = [\"philosophy\", \"philosopher\", \"ethic\", \"morality\", \"metaphysic\", \"epistemology\", \"logic\", \"existentialism\", \"ontology\", \"aesthetic\", \"truth\", \"knowledge\", \"consciousness\", \"free will\", \"rationalism\", \"skepticism\", \"nihilism\", \"stoicism\", \"existentialism\", \"idealism\", \"trascendence\"]\n",
    "how_to_tags = [\"how-to\", \"guide\", \"tutorial\", \"instruction\", \"step-by-step\", \"self-help\", \"manual\", \"handbook\", \"tips\", \"advice\", \"techniques\", \"DIY\", \"lessons\", \"methods\", \"strategies\", \"procedures\", \"skills\", \"training\", \"tactics\", \"expertise\"]\n",
    "spirituality_tags = [\"spirituality\", \"mindfulness\", \"meditation\", \"spiritual\", \"enlightenment\", \"inner peace\", \"consciousness\", \"self-discovery\", \"yoga\", \"soul\", \"journey\", \"wisdom\", \"meditative\", \"yoga\", \"peaceful\", \"serenity\", \"inspiration\"]\n",
    "religion_tags = [\"religion\", \"faith\", \"atheism\", \"belief\", \"theology\", \"god\", \"religious\", \"monotheism\", \"polytheism\", \"scripture\", \"worship\", \"prayer\", \"religious text\", \"religious practices\", \"theological debate\", \"interfaith\", \"sacred\", \"ritual\", \"divinity\", \"pope\", \"bible\"]\n",
    "cook_tags = [\"cooking\", \"culinary\", \"recipes\", \"cookbook\", \"gastronomy\", \"food\", \"culinary art\", \"chef\", \"cuisine\", \"kitchen\", \"techniqus\", \"baking\", \"gourmet\", \"foodie\", \"tasty\", \"cultural cuisine\", \"food science\", \"culinary history\", \"meal\", \"cooking tips\"]\n",
    "classic_tags = [\"classic\", \"vintage\", \"nostalgic\", \"ageless\", \"traditional\", \"heritage\", \"historic\", \"canonical\", \"antique\", \"retro\", \"enduring\", \"beloved\", \"history\", \"mythology\", \"culture\", \"grek\", \"roman\", \" art \", \"archaeology\", \"empire\"]\n",
    "economy_tags = [\"economy\", \"economic\", \"growth\", \"money\",  \"bank\", \"capital\", \"entrepreneur\", \"management\", \"global\", \"industry\", \"policy\", \"finance\", \"business\", \"trend\", \"market\", \"investment\", \"wealth\", \"fiscal\", \"monetary\", \"budgeting\", \"capitalism\", \"consumer\", \"taxation\", \"entrepreneurship\", \"financial\", \"trade\", \"crisis\", \"marketplace\", \"regulation\", \"inflation\", \"deflation\", \"supply\", \"demand\", \"globalization\", \"competition\", \"budget\", \"risk\", \"income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use two parameters that set the number of tags to take per book and per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tag_scores(description):\n",
    "    description_lower = description.lower()\n",
    "    scores = [sum(description_lower.count(word) for word in tag_list) for tag_list in tags_lists]\n",
    "    return pd.Series(scores, index = tag_names)\n",
    "\n",
    "tags_lists = [comedy_tags, fantasy_tags, action_tags, romance_tags, mystery_tags, sci_fi_tags, historical_tags, horror_tags, science_tags, philosophy_tags, how_to_tags, spirituality_tags, religion_tags, cook_tags, classic_tags, economy_tags]\n",
    "tag_names = [tag_list[0] for tag_list in tags_lists]\n",
    "\n",
    "n_tags_per_book = 3\n",
    "n_tags_per_author = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** If you choose to text-mine books.json â descriptions, try to find a way to group books in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5547dee873140f2a8e2835c8dda6ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load books dataset\n",
    "# before we ignored the description column, now we take it together with 'id', 'author_id' and 'title'\n",
    "\n",
    "books_descriptions = pd.DataFrame()\n",
    "\n",
    "chunks_number = np.ceil(7027431 / chunk_size)\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk = chunk[[\"id\", \"author_id\", \"title\", \"description\"]]\n",
    "    books_descriptions = pd.concat([books_descriptions, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdb29b941a545818af74f05a129fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7027431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00b18039a946adb26e24dc2fa578a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7027431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077326</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>[horror, fantasy, mystery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077326</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>[fantasy, action, romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1077326</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>[fantasy, mystery, horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1077326</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>[fantasy, mystery, horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1077326</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>[fantasy, mystery, how-to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25521013</th>\n",
       "      <td>7994</td>\n",
       "      <td>Julie eller Den nya HÃ©loÃ¯se BOK I</td>\n",
       "      <td>[classic, fantasy, romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25521014</th>\n",
       "      <td>3510823</td>\n",
       "      <td>A Frozen Heart</td>\n",
       "      <td>[romance, humor, fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25521015</th>\n",
       "      <td>5614141</td>\n",
       "      <td>O Segredo Da Serpente</td>\n",
       "      <td>[mystery, humor, fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25521016</th>\n",
       "      <td>28525</td>\n",
       "      <td>Lives of Mahomet and his successors</td>\n",
       "      <td>[humor, fantasy, action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25521017</th>\n",
       "      <td>8391847</td>\n",
       "      <td>A Shade For Every Season</td>\n",
       "      <td>[horror, sci-fi, fantasy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7027431 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author_id                                              title  \\\n",
       "id                                                                       \n",
       "2           1077326  Harry Potter and the Order of the Phoenix (Har...   \n",
       "3           1077326  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "4           1077326  Harry Potter and the Chamber of Secrets (Harry...   \n",
       "5           1077326  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "6           1077326  Harry Potter and the Goblet of Fire (Harry Pot...   \n",
       "...             ...                                                ...   \n",
       "25521013       7994                  Julie eller Den nya HÃ©loÃ¯se BOK I   \n",
       "25521014    3510823                                     A Frozen Heart   \n",
       "25521015    5614141                              O Segredo Da Serpente   \n",
       "25521016      28525                Lives of Mahomet and his successors   \n",
       "25521017    8391847                           A Shade For Every Season   \n",
       "\n",
       "                                  tag  \n",
       "id                                     \n",
       "2          [horror, fantasy, mystery]  \n",
       "3          [fantasy, action, romance]  \n",
       "4          [fantasy, mystery, horror]  \n",
       "5          [fantasy, mystery, horror]  \n",
       "6          [fantasy, mystery, how-to]  \n",
       "...                               ...  \n",
       "25521013  [classic, fantasy, romance]  \n",
       "25521014    [romance, humor, fantasy]  \n",
       "25521015    [mystery, humor, fantasy]  \n",
       "25521016     [humor, fantasy, action]  \n",
       "25521017    [horror, sci-fi, fantasy]  \n",
       "\n",
       "[7027431 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "books_tags_scores = books_descriptions[\"description\"].progress_apply(calculate_tag_scores)\n",
    "\n",
    "books_tags_dataset = books_descriptions[[\"id\", \"author_id\", \"title\"]]\n",
    "books_tags_dataset[\"tag\"] = books_tags_scores.progress_apply(lambda x: x.nlargest(n_tags_per_book).index.tolist(), axis=1).values.tolist()\n",
    "\n",
    "books_tags_dataset.set_index(\"id\", inplace = True)\n",
    "books_tags_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:** If you choose to text-mine authors.jsonâ about-field, try to find a way to group authors in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load authors dataset\n",
    "# before we ignored the about column, now we take it together with 'id' and 'name'\n",
    "\n",
    "authors_descriptions = pd.read_json(authors_path, lines = True)\n",
    "authors_descriptions = authors_descriptions[[\"id\", \"name\", \"about\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd0c4508da8426888f52d90b375e5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6285b36258d641b99e18e38214d2d291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>[science, how-to, sci-fi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bill Bryson</td>\n",
       "      <td>[historical, science, classic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jude Fisher</td>\n",
       "      <td>[fantasy, sci-fi, science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>James Hamilton-Paterson</td>\n",
       "      <td>[fantasy, historical, humor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mark Watson</td>\n",
       "      <td>[humor, fantasy, action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777034</th>\n",
       "      <td>Lyon Gamer</td>\n",
       "      <td>[humor, fantasy, action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777055</th>\n",
       "      <td>Brian  Hatch</td>\n",
       "      <td>[humor, fantasy, action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777067</th>\n",
       "      <td>Nicholas Baer</td>\n",
       "      <td>[philosophy, action, historical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777091</th>\n",
       "      <td>James Ketron Ross</td>\n",
       "      <td>[spirituality, fantasy, science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777164</th>\n",
       "      <td>James           Ross</td>\n",
       "      <td>[romance, science, humor]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351767 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name                               tag\n",
       "id                                                                 \n",
       "4                   Douglas Adams         [science, how-to, sci-fi]\n",
       "7                     Bill Bryson    [historical, science, classic]\n",
       "10                    Jude Fisher        [fantasy, sci-fi, science]\n",
       "12        James Hamilton-Paterson      [fantasy, historical, humor]\n",
       "14                    Mark Watson          [humor, fantasy, action]\n",
       "...                           ...                               ...\n",
       "16777034               Lyon Gamer          [humor, fantasy, action]\n",
       "16777055             Brian  Hatch          [humor, fantasy, action]\n",
       "16777067            Nicholas Baer  [philosophy, action, historical]\n",
       "16777091        James Ketron Ross  [spirituality, fantasy, science]\n",
       "16777164     James           Ross         [romance, science, humor]\n",
       "\n",
       "[351767 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_tags_scores = authors_descriptions[\"about\"].progress_apply(calculate_tag_scores)\n",
    "\n",
    "authors_tags_dataset = authors_descriptions[[\"id\", \"name\"]]\n",
    "authors_tags_dataset[\"tag\"] = authors_tags_scores.progress_apply(lambda x: x.nlargest(n_tags_per_author).index.tolist(), axis=1).values.tolist()\n",
    "\n",
    "authors_tags_dataset.set_index(\"id\", inplace = True)\n",
    "authors_tags_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** If you feel comfortable and did both tasks, analyze the matching of the two procedures. You grouped books and authors in genres. Do these two procedures show correspondence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We join the two datasets and search how often the book tag and the author tag correspond. We want to highlight that missing correspondences do not necessarly mean wrong predictions, as an author can be primarly focussed on a certain genre but could also have written different kinds of genres.\n",
    "\n",
    "Before we decided the number of tags to take per book and per author. We say that in general a correspondence is correct is at least one of the tags of the book is in the list of tags of the author. We setted the two parameters with a value of three, but in the next section we also discute about the results with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380142046218597"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.merge(books_tags_dataset, authors_tags_dataset, right_on = \"id\", left_on = \"author_id\", suffixes=('_book', '_author'))\n",
    "correspondence_percentage = merged.apply(lambda row: any(tag in row['tag_author'] for tag in row['tag_book']), axis=1).sum() / books_tags_dataset.shape[0]\n",
    "correspondence_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3 for both parameters makes us obtain a correspondence ratio of 84%, that means that 84% of the times one of the three higher scored tags of the book is between the three higher scored that of the author of that book. We also tried with different kinds of parameters. 3 for books and 1 for author give us a correspondence ratio of 46% while setting both parameters to 1 give us a value of 25%. In all cases the algorithm shows it's potential, given that the accuracy at chance level is 100 / 16 = 6.25%\n",
    "\n",
    "Without further analysis we can't really understand if that percentage is given by a misclassification of books, authors or it's just a natural effect of the fact that authors can write different genres of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Question (CLQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command line is a feature that Data Scientists must master. It is relevant since the operations there require less memory to use in comparison to other interfaces. It also uses less CPU processing time than other interfaces. In addition, it can be faster and more efficient and handle repetitive tasks quickly.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response.\n",
    "\n",
    "Looking through the files, you can find series.json, which contains a list of book series. In each series's 'works' field, you'll find a list of books that are part of that series. Report the title of the top 5 series with the highest total 'books_count' among all of their associated books using command line tools.\n",
    "\n",
    "1. Write a script to provide this report. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_original.[put_the_proper extension]\n",
    "\n",
    "2. Try interacting with ChatGPT or any other LLM chatbot tool to implement a more robust script implementation. Your final script should be at most three lines. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_LLM.[put_the_proper_ extension]. Add in your homework how you employed the LLM chatbot tools, validate if it is correct, and explain how you check its correctness.\n",
    "\n",
    "The expected result is as follows:\n",
    "\n",
    "| id    | title                                | total_books_count |\n",
    "|-------|--------------------------------------|-------------------|\n",
    "| 302380| Extraordinary Voyages                | 20138             |\n",
    "| 94209 | Alice's Adventures in Wonderland     | 14280             |\n",
    "| 311348| Kolekcja ArcydzieÅ Literatury Åwiatowe| 13774             |\n",
    "| 41459 | Oz                                   | 11519             |\n",
    "| 51138 | Hercule Poirot                       | 11305             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1:** Write a script to provide this report. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_original.[put_the_proper extension]\n",
    "\n",
    "---\n",
    "\n",
    "The shell script file *commandline_original.sh* is implemented as requested and the output correspond to what it's expected, the only difference we find it's the transformation of special characters like 'Å' in 'Åwiatowe' to their UNICODE counterpart, as '\\u015'. Comment on the code is inside the file itself but we briefly describe how it works here:\n",
    "\n",
    "For each line of the json file we extract 'id' and 'title' of the series and than the whole \"works\" subsection using Regexes. From the \"works\" subsection we search every field 'work_counts', sum their value, store it in an accumulator and append everything in a list. After having gone through every line of the json file we sort the list using only the total work counts field and using a for cycle print the first five results following the format of the table shown.\n",
    "\n",
    "[INSERT IMAGE OF OUTPUT]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2:** Try interacting with ChatGPT or any other LLM chatbot tool to implement a more robust script implementation. Your final script should be at most three lines. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_LLM.[put_the_proper_ extension]. Add in your homework how you employed the LLM chatbot tools, validate if it is correct, and explain how you check its correctness.\n",
    "\n",
    "---\n",
    "\n",
    "We tried to use ChatGPT 3.5 to implement a more robust script implementation that is also at most three lines. This is the prompt sent:\n",
    "\n",
    "```\n",
    "You have to write a more robust implementation of the given bash code. Your implementation must be at most three lines long. You can't use jq.\n",
    "[original sh code here]\n",
    "```\n",
    "\n",
    "We explicitely said not to use jq to avoid using external libraries. The result is stored in *commandline_LLM.sh* and it doesn't work. We can immediately see it from the different output and the fact that in the source code it's never mentioned the field \"books_count\". We also tried to use alternative LLMs chatbots like Bing with similar results. Similar results are obtained removing the costraint of not using the exernal library jq. If we remove the costraint of using only three lines the results become correct but also extremely similar to the original code. Meanwhile giving only the text of the question does not bring positive results.\n",
    "\n",
    "[TRY DESCRIBING MORE THE PROBLEM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Question (AWSQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS offers access to many cloud-based tools and services that simplify data processing, storage, and analysis. Thanks to AWS's scalable and affordable solutions, data scientists can work effectively with large datasets and carry out advanced analytics. A data scientist must, therefore, perform the essential task of learning how to use AWS. To complete a straightforward data analysis task in this question, you must set up an environment on Amazon Web Services.\n",
    "\n",
    "In this question, you are asked to provide the most commonly used tags for book lists. Going through the list.json file, you'll notice that each list has a list of tags attached, and we want to see what are the most popular tags across all of the lists. Please report the top 5 most frequently used tags and the number of times they appear in the lists.\n",
    "\n",
    "You have to follow the following (recommended) steps:\n",
    "\n",
    "* Download the list.json file to your local system.\n",
    "* Write a Python script that generates the report and the system's time to generate it.\n",
    "* Set up an EC2 instance on your AWS account and upload the list.json file together with your script to the instance\n",
    "* Compare the running times of your script on your local system and the EC2 instances.\n",
    "\n",
    "**Important note:** Please run the same script on both your local system and your EC2 instance to compare the results. e.g., * keep the parameters the same if you are processing the data by loading it partially and aggregating the results. Comment about the differences you find.\n",
    "\n",
    "Please provide a report as follows:\n",
    "\n",
    "* The information about the config of the EC2 instance\n",
    "* The command used to connect to the EC2\n",
    "* The commands used to upload the files and run the script on the EC2 instance through your local system\n",
    "* A table containing the most popular tags and their number of usage\n",
    "* A table containing the running time of the script on your local system and EC2 instance\n",
    "\n",
    "The following is the expected outcome for the most popular tags:\n",
    "\n",
    "|     Tag            |   # Usage   |\n",
    "|--------------------|------------|\n",
    "| romance            |     6001    |\n",
    "| fiction            |     5291    |\n",
    "| young-adult        |     5016    |\n",
    "| fantasy            |     3666    |\n",
    "| science-fiction    |     2779    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "The AWSQ was run on the instance with the following configuration:\n",
    "* Machine type: T3a.large (limited to this size by the plan provided)\n",
    "* Role: LabRole, and access was allowed to an S3 bucket where I had previously stored the list.json file.\n",
    "* RAM: 8Gb\n",
    "* Operative System: Ubuntu\n",
    "\n",
    "The personal computer I was working on (for the sake of performance comparison) has the following specifications:\n",
    "* AMD Ryzen 9 6900Hs cpu\n",
    "* 16 GB DDR5 ram\n",
    "* Windows 11 home\n",
    "\n",
    "The following commands were used to connect to the instance from windows powershell running in administrator mode. Please note that the directories and detailed paths have been replaced by placeholders for the sake of privacy and readibility:\n",
    "1. cd <directory where the key is> (move to the directory where the key is located)\n",
    "2. ssh -i <file.pem> <username@ip-address> (command to connect to instance, obtained from the \"connect\" -> \"ssh\" on the instance page)\n",
    "\n",
    "Now we are connected to the instance, and have switched to using an Ubuntu terminal. To initialize my istance I used the following commands:\n",
    "1. sudo apt update (to check for updates)\n",
    "2. sudo apt upgrade (to install the updates)\n",
    "3. sudo apt install awscli (in order to be to access the contents of my S3 bucket)\n",
    "\n",
    "Note that I already had python3, pip and the libraries installed from a previous run of the instance done during one of the laboratory lectures. Then I downloaded the file from the S3 bucket to use it locally in the EC2 instance. I did that using the command:\n",
    "* aws s3 cp s3://<bucketname> </home/ubuntu/data/list.json>\n",
    "\n",
    "Then I opened a python command line session using the command:\n",
    "* python3\n",
    "\n",
    "And run the following code by pasting it into the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import json\n",
    "\n",
    "start = timer()\n",
    "# STEP ONE: Create an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Define the JSON file path\n",
    "json_file = \"/home/ubuntu/list.json\"\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open(json_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create a DataFrame from the list of data read\n",
    "l_df = pd.DataFrame(data)\n",
    "l_df = l_df[l_df[\"tags\"].notna()]\n",
    "\n",
    "all_tags = [tag for tags in l_df[\"tags\"] for tag in tags]\n",
    "\n",
    "tag_ser = pd.Series(all_tags)\n",
    "\n",
    "# STEP THREE: Count the occurrences of different tags, and get the most recurring 5\n",
    "\n",
    "tag_co = tag_ser.value_counts()\n",
    "top_5 = tag_co.head(5)\n",
    "\n",
    "# End timer and print results\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(\"The top 5 most frequent tags and their respective counts are:\")\n",
    "print(top_5)\n",
    "print(\"This process took\", end - start, \"seconds to finish on the EC2 instance\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code had minor changes when running on my personal computer:\n",
    "```\n",
    "json_file = r\"path\\on\\local\\device\"\n",
    "print(.........\"seconds to finish on my personal device\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome was identical from both, being the following table:\n",
    "\n",
    "| Genre            | Count |\n",
    "|------------------|-------|\n",
    "| romance          | 6001  |\n",
    "| fiction          | 5291  |\n",
    "| young-adult      | 5016  |\n",
    "| fantasy          | 3666  |\n",
    "| science-fiction  | 2779  |\n",
    "\n",
    "The running times however were different:\n",
    "\n",
    "* Personal Device: 29.56 seconds\n",
    "* EC2 instance : 44.67 seconds\n",
    "\n",
    "\n",
    "Please note as well that the process of loading the dataset was broken down into lines since the process was too intensive for the instance to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Question (AQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you are working as a librarian at a public library in Rome. Some new books have arrived, and you are in charge of putting them on a shelf. Your supervisor will give you some instructions, and you will follow his. Each book has a unique ID, and your boss's instructions can be of the following types:\n",
    "\n",
    "* L N - place the book with ID = N on the shelf to the left of the leftmost existing book\n",
    "* R N - place the book with ID = N on the shelf to the right of the rightmost existing book\n",
    "* ? N - Calculate the minimum number of books you must pop from the left or right to have the book with ID = N as the leftmost or rightmost book on the shelf.\n",
    "\n",
    "You must follow your boss's instructions and report the answers to type 3 instructions to him. He guarantees that if he has a type 3 instruction for a book with a specific ID, the book has already been placed on the shelf.\n",
    "\n",
    "Remember that once you've answered a type 3 instruction, the order of the books does not change.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "The first line contains a single number, n, representing the number of your boss's instructions. The ith instruction the boss gives can be found at each of the following n lines.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Print your boss's type 3 instructions in the order they appear in the input.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Input 1\n",
    "```\n",
    "L 75\n",
    "R 20\n",
    "R 30\n",
    "L 11\n",
    "? 75\n",
    "L 12\n",
    "L 15\n",
    "? 20\n",
    "```\n",
    "\n",
    "Output 1\n",
    "```\n",
    "1\n",
    "1\n",
    "```\n",
    "\n",
    "Input 2\n",
    "```\n",
    "R 1\n",
    "L 2\n",
    "L 3\n",
    "L 4\n",
    "? 3\n",
    "R 5\n",
    "R 6\n",
    "L 7\n",
    "L 8\n",
    "? 4\n",
    "L 9\n",
    "R 10\n",
    "R 11\n",
    "L 12\n",
    "L 13\n",
    "? 11\n",
    "? 3\n",
    "```\n",
    "\n",
    "Output 2:\n",
    "```\n",
    "1\n",
    "2\n",
    "0\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1:** Implement a code to answer the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithmic_question(input_string):\n",
    "    input_list = input_string.split(\"\\n\")\n",
    "    n_commands = int(input_list.pop(0))\n",
    "\n",
    "    shelf = deque()\n",
    "    output = []\n",
    "    for i in range(n_commands):\n",
    "        command = input_list[i].split(\" \")\n",
    "        if command[0] == \"L\":\n",
    "            shelf.appendleft(command[1])\n",
    "        elif command[0] == \"R\":\n",
    "            shelf.append(command[1])\n",
    "        elif command[0] == \"?\":\n",
    "            index = shelf.index(command[1])\n",
    "            result = min(index, (len(shelf) - 1) - index)\n",
    "            output.append(result)\n",
    "\n",
    "    print(*output, sep = \"\\n\")\n",
    "    return\n",
    " \n",
    "input1 = \"8\\nL 75\\nR 20\\nR 30\\nL 11\\n? 75\\nL 12\\nL 15\\n? 20\\n\"\n",
    "print(\"Test1:\")\n",
    "algorithmic_question(input1)\n",
    "\n",
    "\n",
    "input2 = \"17\\nR 1\\nL 2\\nL 3\\nL 4\\n? 3\\nR 5\\nR 6\\nL 7\\nL 8\\n? 4\\nL 9\\nR 10\\nR 11\\nL 12\\nL 13\\n? 11\\n? 3\\n\"\n",
    "print(\"\\nTest2:\")\n",
    "algorithmic_question(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the requested algorithm and tested it on the two examples shown. In particular we decided to use a deque as data structure to memorize our values to grant appends to the edges of the list with O(1) time. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2:** Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Do you believe this is correct? How can you double-check it? Elaborate about your answer.\n",
    "\n",
    "---\n",
    "\n",
    "We asked Chat GPT 3.5 for the code's time complexity using the following prompt:\n",
    "\n",
    "```\n",
    "You are given a python code for a function and asked to find its time complexity in the Big-O notation. The input for the function is a string where the first line contains a single number representing the number of instructions. The i-th instruction can be found at each of the following n lines.\n",
    "\n",
    "[python function presented above]\n",
    "```\n",
    "\\\n",
    "The answer provided is:\n",
    "\n",
    "```\n",
    "To determine the time complexity of the given `algorithmic_question` function in Big-O notation, we need to analyze the time complexity of each part of the code separately and then combine them.\n",
    "\n",
    "1. Splitting the input string: \n",
    "   `input_list = input_string.split(\"\\n\")`\n",
    "   This operation splits the input string into a list of lines, which takes O(n) time, where n is the number of characters in the input string.\n",
    "\n",
    "2. Extracting the number of commands:\n",
    "   `n_commands = int(input_list.pop(0))`\n",
    "   `pop(0)` operation on a list has a time complexity of O(n), where n is the number of elements in the list.\n",
    "\n",
    "3. Loop over n_commands (for loop):\n",
    "   The loop iterates n_commands times, and for each iteration, the following operations are performed:\n",
    "   \n",
    "   a. Splitting the command:\n",
    "      `command = input_list[i].split(\" \")`\n",
    "      Splitting a string into a list has a time complexity of O(m), where m is the number of characters in the string.\n",
    "\n",
    "   b. Checking the command type (if-elif statements):\n",
    "      This is a constant-time operation as it involves simple string comparisons.\n",
    "\n",
    "   c. Performing actions based on the command:\n",
    "      - In the case of \"L\" and \"R\" commands, the list `shelf` is modified by adding or removing an element. Appending or inserting an element into a list takes O(1) time on average.\n",
    "      - In the case of the \"?\" command, it searches for an item in the list using the `index` method. In the worst case, this can take O(n) time if the item is at the end of the list.\n",
    "      - Appending to the `output` list takes O(1) time.\n",
    "\n",
    "4. Printing the output:\n",
    "   `print(*output, sep = \"\\n\")`\n",
    "   Printing the output takes O(m) time, where m is the total number of characters in the output.\n",
    "\n",
    "In the worst-case scenario, the most time-consuming operation is searching for an item in the `shelf` list using the `index` method, which has a time complexity of O(n). Therefore, the overall time complexity of the function can be approximated as O(n_commands * n), where n_commands is the number of commands, and n is the maximum number of elements in the `shelf` list during the execution of the function.\n",
    "\n",
    "So, in Big-O notation, the time complexity of the `algorithmic_question` function is O(n * n_commands), where n is the maximum number of elements in the `shelf` list, and n_commands is the number of commands.\n",
    "```\n",
    "\n",
    "\\\n",
    "Chat GPT answers explaining the steps of its reasoning and concluding that the time complexity is O(n * n_commands). Even if that is correct, Chat GPT hasn't grasped that n = n_commands, so we try the same prompt adding a line that makes it more explicit:\n",
    "\n",
    "```\n",
    "You are given a python code for a function and asked to find its time complexity in the Big-O notation. The input for the function is a string where the first line contains a single number representing the number of instructions. The i-th instruction can be found at each of the following n lines. This means that n_commands and n are the same number.\n",
    "\n",
    "[python function presented above]\n",
    "```\n",
    "\\\n",
    "This time the answer of Chat GPT is:\n",
    "\n",
    "```\n",
    "To find the time complexity of the algorithmic_question function in Big-O notation, let's analyze the key operations within the function and count the number of times each operation is executed. We'll break it down step by step:\n",
    "\n",
    "Splitting the input string:\n",
    "\n",
    "The input_string.split(\"\\n\") operation takes O(n) time, where n is the number of lines in the input string.\n",
    "Parsing the number of instructions:\n",
    "\n",
    "Converting the first element of input_list to an integer (int(input_list.pop(0))) takes O(1) time.\n",
    "Looping through the instructions:\n",
    "\n",
    "The loop runs for n_commands iterations, where n_commands is the number of instructions. This loop has a time complexity of O(n_commands).\n",
    "Inside the loop:\n",
    "\n",
    "Splitting the current command: command = input_list[i].split(\" \"). This operation takes O(1) time since the number of words in a command is constant.\n",
    "\n",
    "Checking the command type with if statements: These are O(1) operations since they involve simple string comparisons.\n",
    "\n",
    "Appending to or prepending to the shelf list: These operations involve list manipulation and have a time complexity of O(k), where k is the number of elements in the list. In the worst case, k can be n (number of instructions).\n",
    "\n",
    "Searching for an element in the shelf list: The shelf.index(command[1]) operation takes O(k) time in the worst case, where k is the number of elements in the list.\n",
    "\n",
    "Appending to the output list: This is an O(1) operation.\n",
    "\n",
    "Printing the output list:\n",
    "\n",
    "The print(*output, sep=\"\\n\") operation takes O(k) time, where k is the number of elements in the output list.\n",
    "Now, let's consider the overall time complexity:\n",
    "\n",
    "The most significant part of the time complexity is the loop that runs for n_commands iterations.\n",
    "Within each iteration of the loop, there are O(k) operations, where k is the maximum number of elements in the shelf and output lists.\n",
    "Therefore, the overall time complexity of the algorithmic_question function can be expressed as O(n_commands * k), where n_commands is the number of instructions and k is the maximum number of elements in the shelf and output lists. In the worst case, k can be n (number of instructions), so the time complexity is O(n_commands * n), or simply O(n^2), where n is the number of instructions.\n",
    "\n",
    "So, the time complexity of this function is O(n^2).\n",
    "```\n",
    "\n",
    "\\\n",
    "We belive this is correct and we can double-checking it by doing the calculations ourselves. Given that the deque grants us a O(1) insertion cost on both ends the dominant operation of the algorithm is: \"`index = shelf.index(command[1])`\". This line search the index corresponding to a certain element in a python deque and for doing so it has to check element by element if the object in the deque is the current one. The worst-case scenario for this operation is when the element is found as the last checked element, making the operation runs at O(m) time with m length of the deque.\n",
    "\n",
    "This operation is inside a loop that runs n times, with n the number of operations in the input. We can imagine the worst-case scenario as n/2 \"L\" or \"R\" insertion operations and then n/2 \"?\" search operations, that means that both kinds of operations are called O(n) times. The worst case scenario is if we first insert all the O(n) elements and then do all the O(n) search operations, resulting in a total time complexity of O(n^2).\n",
    "\n",
    "Another way to check whether the cost proposed is true is empirically by testing the algorithm on increasingly bigger inputs and plot its execution time, searching for the corresponding curve.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3:** Is the algorithm proposed in (1.) the optimal one to produce the required output? If not, can you suggest a better algorithm to perform the same task?\n",
    "\n",
    "---\n",
    "\n",
    "The algorithm proposed is optimal in the worst case scenario, but other implementations that uses probabilistic elements may have better average case costs. For example a dictionary has worst case scenario cost for both Get and Set operations of O(n), but thanks to the use of hash tables the average cost is only O(1).\n",
    "\n",
    "We can imagine another algorithm that memorize every inserted element in a dictionary that has ID as key. We don't use indices but instead we say that the first element has value 0 and then we count down when we append to the left (-1, -2) and count up when we append to the right (+1, +2). For doing that we memorize \"min\" and \"max\" values and update them during insertion. When we do a search operation we just return `min((value - min), (max - value))`. This algorithm has an average cost of only O(n), as both insert and search operations are O(1).\n",
    "\n",
    "Other alternatives may be binary search trees, but as dictionaries they also only garantee a better \"average case\" cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
