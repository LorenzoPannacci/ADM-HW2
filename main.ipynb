{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install pandas tqdm numpy matplotlib pyspark dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# GLOBAL PARAMETERS #\n",
    "#####################\n",
    "\n",
    "chunk_size = 100000 # 10000\n",
    "\n",
    "authors_path = r\"datasets/lighter_authors.json\"\n",
    "books_path = r\"datasets/lighter_books.json\"\n",
    "list_path = r\"datasets/list.json\"\n",
    "series_path = r\"datasets/series.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into Pandas' Dataframe\n",
    "\n",
    "The dataset provided is composed of a \"lighter_authors.json\" file of about 0.5 Gbs and a \"lighter_books.json\" file of about 15 Gbs. Considering that where will be a data-type conversion the dataset will become even bigger when loaded on pandas and they could not work on machines with limited amounts of RAM. We can approach this problem from two sides:\n",
    "* Divide the dataset in chunks, work one chunk at a time and merge the result.\n",
    "* For every request we could extract only the columns we are interested with.\n",
    "\n",
    "Both this approach are slow, we have to load every part of the dataset from the storage and load it on RAM for every exercise, and this increase considerably the amount of time to execute each query. Instead we try to load everything all at once, making the dataset lighter by removing columns useless for our analysis and where possible changing the data-type of useful columns to lighter versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the .json file to a pandas dataframe\n",
    "authors = pd.read_json(authors_path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the dataframe\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_authors_memory_usage = authors.memory_usage(index = True, deep = True)\n",
    "raw_authors_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_authors_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_authors_memory_usage[\"about\"] / raw_authors_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"image_url\" and \"about\" are useless for our analysis so they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "authors.drop(columns = [\"image_url\", \"about\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(authors.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] CHANGE DATA TYPES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books\n",
    "\n",
    "The books dataset is much bigger than the authors one and we can't work with it in one go, we have to separate it in chunks. Firstly we analyze what can be done with it by only observing some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a chunk of the dataset from the .json file to a pandas dataframe\n",
    "books = pd.read_json(books_path, lines = True, nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first lines of the chunk\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "raw_books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "raw_books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset just as imported uses\", round(raw_books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")\n",
    "print(\"The 'about' column covers\", round(raw_books_memory_usage[\"description\"] / raw_books_memory_usage.sum(), 2) * 100, \"% of the total RAM usage alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scenario is similar to the authors dataset, there is a column of long text descriptions that occupy a large amount of memory and it's useless to us. We remove it together with other useless columns such as \"image_url\", \"isb\", \"isbn13\", \"asin\" [TODO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the useless columns\n",
    "books.drop(columns = [\"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"publisher\", \"image_url\", \"description\", \"shelves\"], inplace = True)\n",
    "print(\"The dataset now uses\", round(books.memory_usage(index = True, deep = True).sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to load the whole books dataset, chunk by chunk, and removing the useless parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame()\n",
    "\n",
    "chunks_number = np.ceil(7027431 / chunk_size)\n",
    "chunks = pd.read_json(\"datasets/lighter_books.json\", lines = True, chunksize = chunk_size)\n",
    "columns_to_drop = [\"author_name\", \"isbn\", \"isbn13\", \"asin\", \"edition_information\", \"image_url\", \"publisher\", \"shelves\", \"description\"]\n",
    "\n",
    "for chunk in tqdm(chunks, total = chunks_number):\n",
    "    chunk.drop(columns = columns_to_drop, inplace = True)\n",
    "    books = pd.concat([books, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about each column\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some infos about the RAM usage of every column\n",
    "books_memory_usage = books.memory_usage(index = True, deep = True)\n",
    "books_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset uses\", round(books_memory_usage.sum() / 1073741824, 2), \"GBs of RAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions [RQs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ1] Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "In the Authors dataset what's the difference between \"book\" and \"work\"?\n",
    "\n",
    "The Books dataset has some void string entries in the num_pages column.\n",
    "\n",
    "Negative average ratings and ratings count and fans count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ2] Let’s finally dig into this vast dataset, retrieving some vital information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** Plot the number of books for each author in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:**  Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"text_reviews_count\"] >= max(books[\"text_reviews_count\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nlargest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.nsmallest(10, \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.4:** Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.5:** How many books have more than 250 pages?\n",
    "\n",
    "Notice that there are some entries that have a void string instead of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view that excludes the entries with void string\n",
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "# execute query\n",
    "result = df[df[\"num_pages\"].astype(int) > 250].shape[0]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.6:** Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ3] Let’s have a historical look at the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.1:** Write a function that takes as input a year and returns as output the following information:\n",
    "\n",
    "* The number of books published that year.\n",
    "* The total number of pages written that year.\n",
    "* The most prolific month of that year.\n",
    "* The longest book written that year.\n",
    "\n",
    "We have to cope on the number of pages, how do we work in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "def look_by_year(books, year):\n",
    "    books_year = books[\"original_publication_date\" == year]\n",
    "    n_books = books_year.shape[0]\n",
    "\n",
    "    tot_pages = sum(books_year[\"num_pages\"]) #todo\n",
    "    prolific_month = \"\" # todo\n",
    "    longest_book = \"\" #todo\n",
    "\n",
    "    return n_books, tot_pages, prolific_month, longest_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.2:** Use this function to build your data frame: the primary key will be a year, and the required information will be the attributes within the row. Finally, show the head and the tail of this new data frame considering the first ten years registered and the last ten years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3.3:** Ask ChatGPT or any other LLM chatbot tool to implement this function and compare your work with the one the bot gave you as an answer. Does the chatbot implementation work? Please test it out and verify the correctness of the implementation, explaining the process you followed to prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ4] Quirks questions about consistency. In most cases, we will not have a consistent dataset, and the one we are dealing with is no exception. So, let's enhance our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.1:** You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[(authors[\"name\"] == authors[\"name\"]) & (authors[\"id\"] != authors[\"id\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.2:** Write a function that, given a list of author_id, outputs a dictionary where each author_id is a key, and the related value is a list with the names of all the books the author has written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.3:** What is the longest book title among the books of the top 20 authors regarding their average rating? Is it the longest book title overall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 4.4:** What is the shortest overall book title in the dataset? If you find something strange, provide a comment on what happened and an alternative answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ5] We can consider the authors with the most fans to be influential. Let’s have a deeper look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.1a:** Plot the top 10 most influential authors regarding their fan count and number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the actual number of books written by every author using the length of the list of his book ids.\n",
    "authors[\"book_count\"] = authors[\"work_ids\"].apply(len)\n",
    "\n",
    "# 10 most influential authors regarding their fan count\n",
    "x = authors.nlargest(10, \"fans_count\")\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(x[\"name\"], x[\"fans_count\"], color = 'green', label = 'Fan Count')\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(x[\"name\"], x[\"book_count\"], color = 'blue', label = 'Book Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.1b:** Plot the top 10 most influential authors regarding their fan count and number of books. Who is the most influential author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most influential author\n",
    "\n",
    "m_inf = authors.nlargest(1, \"fans_count\")\n",
    "print(\"The most influential author, having by far the largest fanbase is:\", m_inf[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.2:** Have they published any series of books? If any, extract the longest series name among these authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe containing only the books written by the top 10 authors\n",
    "top_authors_books = books[books[\"author_id\"].isin(list(x[\"id\"]))]\n",
    "\n",
    "# get the series authors by their \"series position\" to find who has written series, and use max position to determine their longest series' length.\n",
    "author_series_lengths = top_authors_books.groupby([\"author_id\", \"author_name\"])[\"series_position\"].max().reset_index()\n",
    "\n",
    "# return the highest series position, thus the longest series author and name\n",
    "author_series = top_authors_books.groupby([\"author_id\",\"author_name\", \"series_name\"])[\"series_position\"].max().reset_index()\n",
    "max_series = author_series[author_series[\"series_position\"] == author_series[\"series_position\"].max()]\n",
    "\n",
    "print(\"Authors of series are:\")\n",
    "print(author_series_lengths[\"author_name\"])\n",
    "\n",
    "print(\"\\nThe author of the longest series\")\n",
    "print(max_series[\"author_name\"])\n",
    "print(\"And the series is:\")\n",
    "print(max_series[\"series_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.3:** How many of these authors have been published in different formats? Provide a meaningful chart on the distribution of the formats and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the authors published in different formats from the top 10\n",
    "authors_with_different_formats = top_authors_books[top_authors_books.duplicated(subset=[\"author_id\"], keep=False)]\n",
    "\n",
    "# to avoid name repetition\n",
    "unique_author_ids = authors_with_different_formats[\"author_id\"].unique()\n",
    "\n",
    "# create a separate chart for each author with multiple formats\n",
    "for author_id in unique_author_ids:\n",
    "    author_data = authors_with_different_formats[authors_with_different_formats[\"author_id\"] == author_id]\n",
    "    author_name = author_data[\"author_name\"].values[0]\n",
    "    if not author_data[\"format\"].isnull:\n",
    "        format_distribution = author_data[\"format\"].value_counts()\n",
    "    \n",
    "    format_distribution.plot(kind='bar')\n",
    "    plt.title(f\"Formats Distribution for {author_name}\")\n",
    "    plt.xlabel(\"Format\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on the graphs above:** we can see that the books are mainly published in paperback and hardcover formats, whereas the \"newer\" digital formats are very few. This would be due to 3 main reasons: \n",
    "1- the age of most of the books written by these authors since audiobooks and ebook formats are relatively new.\n",
    "2- even with more widely available digital formats now, these are authors of novels and readers who usually enjoy leisurely reading prefer the physical format of paper, rather than reading through a screen.\n",
    "3- Once a book has been published in a digital format, there is no need to republish it using another editorial, since it is already on the internet, a more universal \"bookstore\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 5.4:** Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender, and comment about anything eventually related to “structural bias.” You may want to have a look at the following recommended readings:\n",
    "\n",
    "- https://bhm.scholasticahq.com/article/38021\n",
    "- https://priyanka-ddit.medium.com/how-to-deal-with-imbalanced-dataset-86de86c49\n",
    "- https://compass.onlinelibrary.wiley.com/doi/10.1111/soc4.12962\n",
    "\n",
    "You can even ask ChatGPT or any other LLM chatbot tool: try to formulate a prompt that provides helpful information about it. Put that information in your notebook and provide comments on what you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ6] For this question, consider the top 10 authors concerning the number of fans again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?\n",
    "    For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 6.1:** Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: transform the publication date entry to datetime format\n",
    "top_authors_books[\"original_publication_date\"] = pd.to_datetime(top_authors_books[\"original_publication_date\"], format = 'mixed')\n",
    "\n",
    "# calculate time gap between consecutive \"original\" publications and add it into a column for all books\n",
    "top_authors_books[\"time_gap\"] = top_authors_books.groupby([\"author_id\", \"series_id\"])[\"original_publication_date\"].diff()\n",
    "\n",
    "# calculate the average time gap for books in a series and not in a series, according to the new column created\n",
    "average_time_gap_series = top_authors_books[top_authors_books[\"series_id\"].notna()][\"time_gap\"].mean()\n",
    "average_time_gap_non_series = top_authors_books[top_authors_books[\"series_id\"].isna()][\"time_gap\"].mean()\n",
    "\n",
    "# print(f\"Average Time Gap for Books in a Series: {average_time_gap_series}\")\n",
    "# print(f\"Average Time Gap for Books Not in a Series: {average_time_gap_non_series}\")\n",
    "print(top_authors_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we expect to see?** As expected the books that belong to a series have a more periodic and regular output from the authors(on average), and this logically makes sense following the idea that the author is breaking down a full story into separate books.\n",
    "Books that do not belong to a series however do not have a well-defined average gap between them since they are independent works and do not follow any periodic output \"expectation\" from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 6.2:** For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new column using the datetime format of the original publication date\n",
    "top_authors_books['year'] = top_authors_books[\"original_publication_date\"].dt.year\n",
    "writers_counts = top_authors_books.groupby(['author_name', 'year']).size().reset_index(name='Count')\n",
    "\n",
    "# plot the publication history for each author\n",
    "writers = writers_counts['author_name'].unique()\n",
    "for writer in writers:\n",
    "    author_data = writers_counts[writers_counts['author_name'] == writer]\n",
    "    plt.plot(author_data['year'], author_data['Count'], label=writer)\n",
    "\n",
    "plt.title(\"Publication History of Authors\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Books Published\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ7] Estimating probabilities is a core skill for a data scientist: show us your best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.1:** Estimate the probability that a book has over 30% of the ratings above 4.\n",
    "\n",
    "We can do that by counting how many books have 30% of the rating above 4 and dividing it by the total number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books[\"rating_dist\"] == \"5:-1|4:1|3:1|2:0|1:0|total:1\"]\n",
    "books[books[\"rating_dist\"] == \"5:0|4:0|3:3|2:-1|1:-1|total:1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_30p_above_four(rating_dist):\n",
    "    pattern = r\"5:([0-9]+)\\|4:[0-9]+\\|3:[0-9]+\\|2:[0-9]+\\|1:[0-9]+\\|total:([0-9]+)\"\n",
    "\n",
    "    match = re.search(pattern, rating_dist)\n",
    "    if int(match.group(1)) > 0.3 * int(match.group(2)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df = books.drop(index = [6650514, 7011288]) # we remove this specific cases because the rating distribution is wrong\n",
    "above_four = sum(df[\"rating_dist\"].apply(has_30p_above_four))\n",
    "\n",
    "result = above_four / df.shape[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.2:** Estimate the probability that an author publishes a new book within two years from its last work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.3:** In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\" Estimate the probability of a book being included in this list, knowing it has more than 700 pages.\n",
    "\n",
    "We call the event \"Book is in the worst books list\" $A$ and the event \"Book has more than 700 pages\" $B$ and we use Bayes' formula:\n",
    "\n",
    "$\\mathbb{P}(\\text{A | B}) = \\frac{\\mathbb{P}(\\text{B | A}) \\;\\cdot\\; \\mathbb{P}(\\text{A}) }{\\mathbb{P}(\\text{B})}$\n",
    "\n",
    "* $\\mathbb{P}(\\text{A})$: the probability that a book is in the worst books list. Obtained by counting the entries in \"The Worst Books of All Time\" list and dividing it by the number of books in the dataset.\n",
    "* $\\mathbb{P}(\\text{B})$: the probability that a book has more than 700 pages. Obtained by counting the entries with more than 700 pages and dividing it by the number of books in the dataset.\n",
    "* $\\mathbb{P}(\\text{B | A})$: the probability of a book having more than 700 pages knowing that it is in the worst book list. Obtained by doing the second query over the books identified by first one.\n",
    "\n",
    "\n",
    "Se i dataset sono in parte disgiunti che senso ha questa query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = books[books[\"num_pages\"] != \"\"]\n",
    "\n",
    "worst_books = pd.read_json(list_path, lines = True, nrows = 1)[\"books\"][0]\n",
    "worst_books = pd.DataFrame(worst_books)\n",
    "worst_books[\"book_id\"] = worst_books[\"book_id\"].astype(int)\n",
    "n_worst_books = worst_books.shape[0]\n",
    "join = pd.merge(worst_books, df, left_on = \"book_id\", right_on = \"id\", how = \"inner\", suffixes = [\"_w\", \"_b\"])\n",
    "\n",
    "pA = n_worst_books / df.shape[0]\n",
    "pB = df[df[\"num_pages\"].astype(int) > 700].shape[0] / (df.shape[0])\n",
    "pBA = join[join[\"num_pages\"].astype(int) > 700].shape[0] / (join.shape[0])\n",
    "pAB = pBA * pA / pB\n",
    "\n",
    "print(\"pA: \", pA)\n",
    "print(\"pB: \", pB)\n",
    "print(\"pBA: \", pBA)\n",
    "print(\"pAB: \", pAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 7.4:** Are the events X=’Being Included in The Worst Books of All Time list’ and Y=’Having more than 700 pages’ independent? Explain how you have obtained your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RQ8] Charts, statistical tests, and analysis methods are splendid tools to illustrate your data-driven decisions to check whether a hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.1:** Can you demonstrate that readers usually rate the longest books as the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.2:** Compare the average rate distribution for English and non-English books with a proper statistical procedure. What can you conclude about those two groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.3:** About the two groups in the previous question, extract helpful statistics like mode, mean, median, and quartiles, explaining their role in a box plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.4:** It seems reasonable to assume that authors with more fans should have more reviews, but maybe their fans are a bit lazy. Confirm or reject this with a convenient statistical test or a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 8.5:** Provide a short survey about helpful statistical tests in data analysis and mining: focus on hypothesis design and the difference between parametric and nonparametric tests, explaining the reasons behind the choice of one of these two tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.1:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload authors.json dataset, and filter authors with at least 100 reviews. Do the same using Pandas and compare performance in terms of milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PANDAS\n",
    "\n",
    "print(\"Measuring Pandas performance...\")\n",
    "%timeit authors[authors[\"text_reviews_count\"] >= 100]\n",
    "\n",
    "# IN DASK\n",
    "\n",
    "authors_dask = dd.from_pandas(authors, npartitions = 1)\n",
    "\n",
    "print(\"Measuring Dask performance...\")\n",
    "%timeit authors_dask[authors_dask[\"text_reviews_count\"] >= 100].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1.2:** Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload books.json, and join them with authors.json based on author_id. How many books don’t have a match for the author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = pd.merge(books, authors, left_on = \"author_id\", right_on = \"id\", how = \"left\")[[\"id_y\"]]\n",
    "authorless_istances = join[\"id_y\"].isna().sum()\n",
    "print(\"Authorless istances: \", authorless_istances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = dd.merge(dd.from_pandas(books, npartitions = 1), dd.from_pandas(authors, npartitions = 1), left_on = \"author_id\", right_on = \"id\", how = \"left\")\n",
    "authorless_instances = join[\"id_y\"].isna().sum().compute()\n",
    "print(\"Authorless instances: \", authorless_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del worst_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Every book should have a field named description, and any author should have a field named description. Choose one of the two and perform a text-mining analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.1:** If you choose to text-mine books.json ’ descriptions, try to find a way to group books in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.2:** If you choose to text-mine authors.json’ about-field, try to find a way to group authors in genres using whatever procedure you want, highlighting words that are triggers for these choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2.3:** If you feel comfortable and did both tasks, analyze the matching of the two procedures. You grouped books and authors in genres. Do these two procedures show correspondence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Question (CLQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command line is a feature that Data Scientists must master. It is relevant since the operations there require less memory to use in comparison to other interfaces. It also uses less CPU processing time than other interfaces. In addition, it can be faster and more efficient and handle repetitive tasks quickly.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response.\n",
    "\n",
    "Looking through the files, you can find series.json, which contains a list of book series. In each series's 'works' field, you'll find a list of books that are part of that series. Report the title of the top 5 series with the highest total 'books_count' among all of their associated books using command line tools.\n",
    "\n",
    "1. Write a script to provide this report. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_original.[put_the_proper extension]\n",
    "\n",
    "2. Try interacting with ChatGPT or any other LLM chatbot tool to implement a more robust script implementation. Your final script should be at most three lines. Put your script in a shell script file with the appropriate extension, then run it from the command line. The file should be called commandline_LLM.[put_the_proper_ extension]. Add in your homework how you employed the LLM chatbot tools, validate if it is correct, and explain how you check its correctness.\n",
    "\n",
    "The expected result is as follows:\n",
    "\n",
    "| id    | title                                | total_books_count |\n",
    "|-------|--------------------------------------|-------------------|\n",
    "| 302380| Extraordinary Voyages                | 20138             |\n",
    "| 94209 | Alice's Adventures in Wonderland     | 14280             |\n",
    "| 311348| Kolekcja Arcydzieł Literatury Światowe| 13774             |\n",
    "| 41459 | Oz                                   | 11519             |\n",
    "| 51138 | Hercule Poirot                       | 11305             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1:**\n",
    "\n",
    "The shell script file *commandline_original.sh* is implemented as requested and the output correspond to what it's expected, the only difference we find it's the transformation of special characters like 'Ś' in 'Światowe' to their unicode counterpart, as '\\u015'. Comment on the code is inside the file itself but we briefly describe the code here: for each line of the json file we extract 'id' and 'title' of the series and than the whole \"works\" subsection using Regexs. From the \"works\" subsection we search every field 'work_counts', sum the value, store it in an accumulator and append everything in a list. After having gone through every line of the json file we sort the list using only the total work counts field and using a for cycle print the first five results following the format of the table shown.\n",
    "\n",
    "**Request 2:**\n",
    "\n",
    "We tried to use ChatGPT 3.5 to implement a more robust script implementation that is also at most three lines. This is the prompt sent:\n",
    "\n",
    "```\n",
    "You have to write a more robust implementation of the given bash code. Your implementation must be at most three lines long. You can't use jq.\n",
    "[original sh code here]\n",
    "```\n",
    "\n",
    "We explicitely said not to use jq to avoid using external libraries. The result is stored in *commandline_LLM.sh* and it doesn't work. We can immediately see it from the different output and the fact that in the source code it's never mentioned the field \"books_count\". We also tried to use alternative LLMs chatbots like Bing with similar results. Removing the costraint of using only three lines the results become correct but also extremely similar to the original code. Meanwhile giving only the text of the question does not bring positive results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Question (AWSQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS offers access to many cloud-based tools and services that simplify data processing, storage, and analysis. Thanks to AWS's scalable and affordable solutions, data scientists can work effectively with large datasets and carry out advanced analytics. A data scientist must, therefore, perform the essential task of learning how to use AWS. To complete a straightforward data analysis task in this question, you must set up an environment on Amazon Web Services.\n",
    "\n",
    "In this question, you are asked to provide the most commonly used tags for book lists. Going through the list.json file, you'll notice that each list has a list of tags attached, and we want to see what are the most popular tags across all of the lists. Please report the top 5 most frequently used tags and the number of times they appear in the lists.\n",
    "\n",
    "You have to follow the following (recommended) steps:\n",
    "\n",
    "* Download the list.json file to your local system.\n",
    "* Write a Python script that generates the report and the system's time to generate it.\n",
    "* Set up an EC2 instance on your AWS account and upload the list.json file together with your script to the instance\n",
    "* Compare the running times of your script on your local system and the EC2 instances.\n",
    "\n",
    "**Important note:** Please run the same script on both your local system and your EC2 instance to compare the results. e.g., * keep the parameters the same if you are processing the data by loading it partially and aggregating the results. Comment about the differences you find.\n",
    "\n",
    "Please provide a report as follows:\n",
    "\n",
    "* The information about the config of the EC2 instance\n",
    "* The command used to connect to the EC2\n",
    "* The commands used to upload the files and run the script on the EC2 instance through your local system\n",
    "* A table containing the most popular tags and their number of usage\n",
    "* A table containing the running time of the script on your local system and EC2 instance\n",
    "\n",
    "The following is the expected outcome for the most popular tags:\n",
    "\n",
    "|     Tag            |   # Usage   |\n",
    "|--------------------|------------|\n",
    "| romance            |     6001    |\n",
    "| fiction            |     5291    |\n",
    "| young-adult        |     5016    |\n",
    "| fantasy            |     3666    |\n",
    "| science-fiction    |     2779    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Question (AQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you are working as a librarian at a public library in Rome. Some new books have arrived, and you are in charge of putting them on a shelf. Your supervisor will give you some instructions, and you will follow his. Each book has a unique ID, and your boss's instructions can be of the following types:\n",
    "\n",
    "* L N - place the book with ID = N on the shelf to the left of the leftmost existing book\n",
    "* R N - place the book with ID = N on the shelf to the right of the rightmost existing book\n",
    "* ? N - Calculate the minimum number of books you must pop from the left or right to have the book with ID = N as the leftmost or rightmost book on the shelf.\n",
    "\n",
    "You must follow your boss's instructions and report the answers to type 3 instructions to him. He guarantees that if he has a type 3 instruction for a book with a specific ID, the book has already been placed on the shelf.\n",
    "\n",
    "Remember that once you've answered a type 3 instruction, the order of the books does not change.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "The first line contains a single number, n, representing the number of your boss's instructions. The ith instruction the boss gives can be found at each of the following n lines.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Print your boss's type 3 instructions in the order they appear in the input.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Input 1\n",
    "```\n",
    "L 75\n",
    "R 20\n",
    "R 30\n",
    "L 11\n",
    "? 75\n",
    "L 12\n",
    "L 15\n",
    "? 20\n",
    "```\n",
    "\n",
    "Output 1\n",
    "```\n",
    "1\n",
    "1\n",
    "```\n",
    "\n",
    "Input 2\n",
    "```\n",
    "R 1\n",
    "L 2\n",
    "L 3\n",
    "L 4\n",
    "? 3\n",
    "R 5\n",
    "R 6\n",
    "L 7\n",
    "L 8\n",
    "? 4\n",
    "L 9\n",
    "R 10\n",
    "R 11\n",
    "L 12\n",
    "L 13\n",
    "? 11\n",
    "? 3\n",
    "```\n",
    "\n",
    "Output 2:\n",
    "```\n",
    "1\n",
    "2\n",
    "0\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 1:** Implement a code to answer the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithmic_question(input_string):\n",
    "    input_list = input_string.split(\"\\n\")\n",
    "    n_commands = int(input_list.pop(0))\n",
    "\n",
    "    shelf = []\n",
    "    output = []\n",
    "    for i in range(n_commands):\n",
    "        command = input_list[i].split(\" \")\n",
    "        if command[0] == \"L\":\n",
    "            shelf = [command[1]] + shelf\n",
    "        elif command[0] == \"R\":\n",
    "            shelf = shelf + [command[1]]\n",
    "        elif command[0] == \"?\":\n",
    "            index = shelf.index(command[1])\n",
    "            result = min(index, (len(shelf) - 1) - index)\n",
    "            output.append(result)\n",
    "\n",
    "    print(*output, sep = \"\\n\")\n",
    "    return\n",
    " \n",
    "input1 = \"8\\nL 75\\nR 20\\nR 30\\nL 11\\n? 75\\nL 12\\nL 15\\n? 20\\n\"\n",
    "print(\"Test1:\")\n",
    "algorithmic_question(input1)\n",
    "\n",
    "\n",
    "input2 = \"17\\nR 1\\nL 2\\nL 3\\nL 4\\n? 3\\nR 5\\nR 6\\nL 7\\nL 8\\n? 4\\nL 9\\nR 10\\nR 11\\nL 12\\nL 13\\n? 11\\n? 3\\n\"\n",
    "print(\"\\nTest2:\")\n",
    "algorithmic_question(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 2:** Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Do you believe this is correct? How can you double-check it? Elaborate about your answer.\n",
    "\n",
    "We asked Chat GPT 3.5 for the code's time complexity using the following prompt:\n",
    "\n",
    "```\n",
    "You are given a python code for a function and asked to find its time complexity in the Big-O notation. The input for the function is a string where the first line contains a single number representing the number of instructions. The i-th instruction can be found at each of the following n lines.\n",
    "\n",
    "[python function presented above]\n",
    "```\n",
    "\\\n",
    "The answer provided is:\n",
    "\n",
    "```\n",
    "To determine the time complexity of the given `algorithmic_question` function in Big-O notation, we need to analyze the time complexity of each part of the code separately and then combine them.\n",
    "\n",
    "1. Splitting the input string: \n",
    "   `input_list = input_string.split(\"\\n\")`\n",
    "   This operation splits the input string into a list of lines, which takes O(n) time, where n is the number of characters in the input string.\n",
    "\n",
    "2. Extracting the number of commands:\n",
    "   `n_commands = int(input_list.pop(0))`\n",
    "   `pop(0)` operation on a list has a time complexity of O(n), where n is the number of elements in the list.\n",
    "\n",
    "3. Loop over n_commands (for loop):\n",
    "   The loop iterates n_commands times, and for each iteration, the following operations are performed:\n",
    "   \n",
    "   a. Splitting the command:\n",
    "      `command = input_list[i].split(\" \")`\n",
    "      Splitting a string into a list has a time complexity of O(m), where m is the number of characters in the string.\n",
    "\n",
    "   b. Checking the command type (if-elif statements):\n",
    "      This is a constant-time operation as it involves simple string comparisons.\n",
    "\n",
    "   c. Performing actions based on the command:\n",
    "      - In the case of \"L\" and \"R\" commands, the list `shelf` is modified by adding or removing an element. Appending or inserting an element into a list takes O(1) time on average.\n",
    "      - In the case of the \"?\" command, it searches for an item in the list using the `index` method. In the worst case, this can take O(n) time if the item is at the end of the list.\n",
    "      - Appending to the `output` list takes O(1) time.\n",
    "\n",
    "4. Printing the output:\n",
    "   `print(*output, sep = \"\\n\")`\n",
    "   Printing the output takes O(m) time, where m is the total number of characters in the output.\n",
    "\n",
    "In the worst-case scenario, the most time-consuming operation is searching for an item in the `shelf` list using the `index` method, which has a time complexity of O(n). Therefore, the overall time complexity of the function can be approximated as O(n_commands * n), where n_commands is the number of commands, and n is the maximum number of elements in the `shelf` list during the execution of the function.\n",
    "\n",
    "So, in Big-O notation, the time complexity of the `algorithmic_question` function is O(n * n_commands), where n is the maximum number of elements in the `shelf` list, and n_commands is the number of commands.\n",
    "```\n",
    "\n",
    "\\\n",
    "Chat GPT answers, explaining the steps of its reasoning and concluding that the time complexity is O(n * n_commands). Even if that is correct, Chat GPT hasn't grasped that n = n_commands, so we try the same prompt adding a line that makes it more explicit:\n",
    "\n",
    "```\n",
    "You are given a python code for a function and asked to find its time complexity in the Big-O notation. The input for the function is a string where the first line contains a single number representing the number of instructions. The i-th instruction can be found at each of the following n lines. This means that n_commands and n are the same number.\n",
    "\n",
    "[python function presented above]\n",
    "```\n",
    "\\\n",
    "This time the answer of Chat GPT is:\n",
    "\n",
    "```\n",
    "To find the time complexity of the algorithmic_question function in Big-O notation, let's analyze the key operations within the function and count the number of times each operation is executed. We'll break it down step by step:\n",
    "\n",
    "Splitting the input string:\n",
    "\n",
    "The input_string.split(\"\\n\") operation takes O(n) time, where n is the number of lines in the input string.\n",
    "Parsing the number of instructions:\n",
    "\n",
    "Converting the first element of input_list to an integer (int(input_list.pop(0))) takes O(1) time.\n",
    "Looping through the instructions:\n",
    "\n",
    "The loop runs for n_commands iterations, where n_commands is the number of instructions. This loop has a time complexity of O(n_commands).\n",
    "Inside the loop:\n",
    "\n",
    "Splitting the current command: command = input_list[i].split(\" \"). This operation takes O(1) time since the number of words in a command is constant.\n",
    "\n",
    "Checking the command type with if statements: These are O(1) operations since they involve simple string comparisons.\n",
    "\n",
    "Appending to or prepending to the shelf list: These operations involve list manipulation and have a time complexity of O(k), where k is the number of elements in the list. In the worst case, k can be n (number of instructions).\n",
    "\n",
    "Searching for an element in the shelf list: The shelf.index(command[1]) operation takes O(k) time in the worst case, where k is the number of elements in the list.\n",
    "\n",
    "Appending to the output list: This is an O(1) operation.\n",
    "\n",
    "Printing the output list:\n",
    "\n",
    "The print(*output, sep=\"\\n\") operation takes O(k) time, where k is the number of elements in the output list.\n",
    "Now, let's consider the overall time complexity:\n",
    "\n",
    "The most significant part of the time complexity is the loop that runs for n_commands iterations.\n",
    "Within each iteration of the loop, there are O(k) operations, where k is the maximum number of elements in the shelf and output lists.\n",
    "Therefore, the overall time complexity of the algorithmic_question function can be expressed as O(n_commands * k), where n_commands is the number of instructions and k is the maximum number of elements in the shelf and output lists. In the worst case, k can be n (number of instructions), so the time complexity is O(n_commands * n), or simply O(n^2), where n is the number of instructions.\n",
    "\n",
    "So, the time complexity of this function is O(n^2).\n",
    "```\n",
    "\n",
    "\\\n",
    "We belive this is correct and we can double-checking it by doing the calculations ourselves. The dominant operation of the algorithm is: \"`index = shelf.index(command[1])`\". This line search the index corresponding to a certain element in a python list and for doing so it has to check element by element if the element in the list is the current one. The worst-case scenario for this operation is when the element is found as the last checked element, making the operation runs at O(m) time with m length of the list.\n",
    "\n",
    "This operation is inside a loop that runs n times, with n the number of operations in the input. We can imagine the worst-case scenario as n/2 \"L\" or \"R\" insertion operations and then n/2 \"?\" search operations, which both are O(n). This means that we will have  n elements in the array and doing the search operations n times, resulting in a total time complexity of O(n^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request 3:** Is the algorithm proposed in (1.) the optimal one to produce the required output? If not, can you suggest a better algorithm to perform the same task?\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
